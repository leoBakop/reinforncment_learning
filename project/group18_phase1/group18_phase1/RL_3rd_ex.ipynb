{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##POKER PLAYING AGENTS\n",
        "Group 18 \\\n",
        "Leonidas Bakopoulos AM 2018030036 \\\n",
        "Alexandra Tsipouraki AM 2018030089\n",
        "\n",
        "\n",
        "To execute the code, run all the cells in the order they are displayed below (press Runtime -> Run all from the main Colab menu)\n",
        "\n",
        "##Plotting\n",
        "To produce and see ALL the plots that are shown in the report please run our main.py file in VS Code but for all the possible values(True or False) of these parameters (last lines of our main in google colab too) :  \n",
        "    q_learning = True #Use a q-learning agent or not\\\n",
        "    threshold = True #Use a threshold or a random opponent \\\n",
        "    aggressive = True#in case of threshold, use aggressive or\\ defensive opponent\n",
        "\n",
        "Then run plotting.py from the 'project' folder\n",
        "\n",
        "To play against Q-learning please set train = False"
      ],
      "metadata": {
        "id": "qrM4Ljs7qeXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4qd7Wdl9hY",
        "outputId": "1429db86-fc72-4391-b75c-fcee9d939542"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./images"
      ],
      "metadata": {
        "id": "d0otTHmam-0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa9ad99-dd0b-4d3d-dceb-d2438aba032e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./images’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agents cell\n",
        "The code below can be also found in : agent.py, implemented_agents.py\n"
      ],
      "metadata": {
        "id": "-d5gR-LRi5Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def send_action(self, state):\n",
        "        return NotImplemented\n",
        "\n",
        "    def to_str(self):\n",
        "        return \"Agent\"\n",
        "    def reduce_a(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class PolicyIterationAgent(Agent):\n",
        "    def __init__(self,P, epsilon = 10**(-4), gamma=.9):\n",
        "        self.P = P #Transition Matrix\n",
        "        self.epsilon = epsilon #conergence criterion\n",
        "        self.gamma = gamma\n",
        "        self.pi = None #initial policies\n",
        "        self.V = None #initial V\n",
        "        self.V, self.pi = self.policy_iteration(self.gamma) #converged policies and V\n",
        "        return\n",
        "\n",
        "\n",
        "    def send_action(self, state):\n",
        "        \"\"\"\n",
        "        Method that every agent inherits.\n",
        "        Is used by agent, for deciding the best action\n",
        "        \"\"\"\n",
        "        return  self.pi(state)\n",
        "\n",
        "    def policy_evaluation(self, pi = None):\n",
        "        self.pi = pi if pi is not None else self.pi\n",
        "        prev_V = np.zeros(len(self.P)) # use as \"cost-to-go\", i.e. for V(s'):\n",
        "        while True:\n",
        "            V = np.zeros(len(self.P)) # current value function to be learnerd\n",
        "            for s in range(len(self.P)):  # do for every state\n",
        "\n",
        "                for prob, next_state, reward, done in self.P[s][self.pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
        "                    V[s] += prob * (reward + self.gamma * prev_V[next_state] * (not done))\n",
        "            if np.max(np.abs(prev_V - V)) < self.epsilon: #check if the new V estimate is close enough to the previous one;\n",
        "                break\n",
        "            prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
        "\n",
        "        self.V = V\n",
        "        return\n",
        "\n",
        "    def policy_improvement(self, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
        "        Q = np.zeros((len(self.P), len(self.P[0])), dtype=np.float64) #create a Q value array\n",
        "        for s in range(len(self.P)):        # for every state in the environment/model\n",
        "            for a in range(len(self.P[s])):  # and for every action in that state\n",
        "                for prob, next_state, reward, done in self.P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve)\n",
        "                    Q[s][a] += prob * (reward + gamma * self.V[next_state] * (not done))\n",
        "        new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
        "\n",
        "        self.pi = new_pi\n",
        "        return new_pi\n",
        "\n",
        "    def policy_iteration(self, gamma):\n",
        "        t = 0\n",
        "        random_actions = np.random.choice(tuple(self.P[0].keys()), len(self.P))     # start with random actions for each state\n",
        "        self.pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
        "\n",
        "        while True:\n",
        "            old_pi = {s: self.pi(s) for s in range(len(self.P))}  #keep the old policy to compare with new\n",
        "            self.policy_evaluation()   #evaluate latest policy --> you receive its converged value function\n",
        "            self.pi = self.policy_improvement(gamma=gamma)          #get a better policy using the value function of the previous one just calculated\n",
        "\n",
        "            t += 1\n",
        "\n",
        "            if old_pi == {s:self.pi(s) for s in range(len(self.P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
        "                break\n",
        "        print('converged after %d iterations' %t) #keep track of the number of (outer) iterations to converge\n",
        "        return self.V,self.pi\n",
        "\n",
        "\n",
        "class Q_Learning_Agent(Agent):\n",
        "\n",
        "    def __init__(self, state_size, action_size=3, a = .2, gamma = 1.0, seed = 0, Q=None, eps=1, against_human = False):\n",
        "        \"\"\"\n",
        "        There are two ways to initialize the Q_Learning_Agent\n",
        "        a) By not given an pre-trained Q (and the goal is to create it)\n",
        "        b) By given a pre-trained Q (and the goal is to test it)\n",
        "        \"\"\"\n",
        "        np.random.seed(seed=seed)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.a = a\n",
        "\n",
        "        if Q is not None:self.Q = Q\n",
        "        else: self.Q = np.random.rand(self.state_size, self.action_size)\n",
        "\n",
        "        self.conv = 0\n",
        "        self.eps = eps\n",
        "        self.disp = True\n",
        "\n",
        "        self.against_human  = against_human #means that the agent is in testing mode\n",
        "\n",
        "    def train(self, tuple):\n",
        "        \"\"\"\n",
        "            The most basic method of the Q-Agent.\n",
        "            In this method, the training is implemented\n",
        "            based on the previous tuple (knowledge)\n",
        "        \"\"\"\n",
        "\n",
        "        old_q = list([np.argmax(i) for i in self.Q]) #in order to check for convergance\n",
        "        state, prev_action, reward, next_state,  done = tuple\n",
        "        if not prev_action is None: #it is None when the agent talks first\n",
        "            target = reward + self.gamma*np.argmax(self.Q[next_state])*(not done) #bellman equastion\n",
        "            self.Q[state, prev_action]= (1-self.a)*self.Q[state, prev_action] + self.a*target #learn with learning rate = a\n",
        "\n",
        "        #convergences stuff\n",
        "        if(self.check_if_convergence(old_q)):self.conv += 1\n",
        "        else: self.conv = 0\n",
        "        if  (self.conv == 10_000):\n",
        "            print(old_q)\n",
        "        return (self.conv == 10_000)\n",
        "\n",
        "\n",
        "    def check_if_convergence(self, old_q):\n",
        "        \"\"\"\n",
        "            Method that compares two instances of policies\n",
        "        \"\"\"\n",
        "        new_q = list([np.argmax(i) for i in self.Q])\n",
        "        for i,j in zip(old_q,new_q):\n",
        "            if(i!=j):return False\n",
        "        return True\n",
        "\n",
        "\n",
        "    def send_action(self, state):\n",
        "        \"\"\"\n",
        "        Method that every agent inherits.\n",
        "        Is used by agent, for deciding the best action.\n",
        "        In this case, an epsilon greedy algorithm is used,\n",
        "        in order to achieve exploration and exploitation\n",
        "        \"\"\"\n",
        "\n",
        "        self.eps = max(0.9999749*self.eps, .01) #was choosen experimentally, in oder to achieve .01 in 26% of\n",
        "        #the horizon in episodes against threshold(s) opponent(s)\n",
        "\n",
        "        if self.against_human: #In order of pretained\n",
        "            self.eps = .001\n",
        "\n",
        "\n",
        "\n",
        "        if self.eps < .0101 and self.disp:\n",
        "            print(\"eps = .01------------\")\n",
        "            self.disp = False\n",
        "        p = np.random.rand()\n",
        "\n",
        "        #select action\n",
        "        if p < self.eps: #make a random move\n",
        "            action = np.random.choice([0,1,2])\n",
        "        else : #act as q suggests\n",
        "            action = np.argmax(self.Q[state,:])\n",
        "\n",
        "        #previous bug.\n",
        "        #It was fixed (we can see that nothing is pinted) but wasn't deleted for safety reasons\n",
        "        if ((not (action in [0,1,2])) and ( action is not None)):\n",
        "            print(\"bug needs to be fixed\")\n",
        "            return np.random.choice([0,1,2])\n",
        "        return action\n",
        "\n",
        "    def to_str(self):\n",
        "        return \"Q_Learning_Agent\"\n",
        "\n",
        "    def reduce_a(self):\n",
        "        \"\"\" reducing the learning rate, as it is mentioned in report\"\"\"\n",
        "        self.a = min(0.99998*self.a, 0.12)\n",
        "\n",
        "\n",
        "class Random_Agent(Agent):\n",
        "    def __init__(self, seed = 0):\n",
        "        np.random.seed(seed = seed)\n",
        "\n",
        "    def send_action(self, state):\n",
        "        \"\"\" the extra arguments, are not used\n",
        "            They are completed just for constistency\n",
        "        \"\"\"\n",
        "        return np.random.randint(0,3)\n",
        "\n",
        "\n",
        "\n",
        "class Threshold_Agent_D(Agent):\n",
        "    \"\"\"\n",
        "    This is the tight opponent\n",
        "    \"\"\"\n",
        "    def set_hand(self, hand):\n",
        "        self.hand = hand\n",
        "    def set_table(self, table):\n",
        "        self.table = table\n",
        "    def set_round(self, round):\n",
        "        self.round = round\n",
        "    def send_action(self, state):\n",
        "\n",
        "        cards_and_actions_round_0 = {\n",
        "            \"T\": 1,\n",
        "            \"J\": 1,\n",
        "            \"Q\":0,\n",
        "            \"K\":0,\n",
        "            \"A\": 2\n",
        "        }\n",
        "\n",
        "        #in preflop just act based on the rank of your hand\n",
        "        if self.round == 0:\n",
        "            return cards_and_actions_round_0.get(self.hand.rank, 0) #by default check\n",
        "        #in flop, action is based on the combination\n",
        "        for card in self.table:\n",
        "            if card.rank == self.hand.rank: return 2\n",
        "        #if no combination in the flop, fold\n",
        "        return 1\n",
        "\n",
        "\n",
        "class Threshold_Agent_A(Threshold_Agent_D):\n",
        "    \"\"\"\n",
        "    This is the loose opponent\n",
        "    \"\"\"\n",
        "    def send_action(self, state):\n",
        "\n",
        "        cards_and_actions_round_0 = {\n",
        "            \"T\": 0,\n",
        "            \"J\": 0,\n",
        "            \"Q\":2,\n",
        "            \"K\":2,\n",
        "            \"A\": 2\n",
        "        }\n",
        "        #in preflop just act based on the rank of your hand\n",
        "        if self.round == 0:\n",
        "            return cards_and_actions_round_0.get(self.hand.rank, 0) #by default check\n",
        "        #in flop, action is based on the combination\n",
        "        for card in self.table:\n",
        "            if card.rank == self.hand.rank: return 2\n",
        "        #if no combination in the flop, check\n",
        "        return 0\n",
        "\n",
        "class Human_Agent(Agent):\n",
        "    \"\"\"\n",
        "    Agent for testing purposes.\n",
        "    This agent, implements the inteface between\n",
        "    the python code and the tester\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_size, threshold = False, ante = False):\n",
        "        self.action_size = action_size\n",
        "        self.disp = True\n",
        "        self.threshold = threshold\n",
        "        self.ante=ante\n",
        "\n",
        "    def set_hand(self, hand):\n",
        "        self.hand = hand\n",
        "\n",
        "\n",
        "    def set_table(self, table):\n",
        "        self.table = table\n",
        "\n",
        "    def set_round(self, round):\n",
        "        self.round = round\n",
        "\n",
        "    def interface_display(self):\n",
        "        print(f\"My current hand is : {self.hand}\")\n",
        "        if not( self.table[0].rank == '-1'):\n",
        "            print(f\"The table has :{self.table[0].rank} , {self.table[1].rank}\")\n",
        "        print(f\"We are playing round number {self.round} of the game\")\n",
        "\n",
        "    def send_action(self, state):\n",
        "        # Implement the logic to receive the action from the human player\n",
        "        # Return the selected action\n",
        "        self.interface_display()\n",
        "        valid_actions = [0, 1, 2]\n",
        "        while True:\n",
        "            action = input(\"Enter your action (for quitting press q): Press 0 to 'check' or 'call', 1 to 'fold' and 2 to 'raise': \")\n",
        "            if action.isdigit() and int(action) in valid_actions:\n",
        "                return int(action)\n",
        "            elif action.lower() == 'q':\n",
        "                sys.exit(0)\n",
        "            else:\n",
        "                 print(\"Invalid input. Please enter a valid action.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TFXds1xWfiJ-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Card"
      ],
      "metadata": {
        "id": "ve7F2PAzi_as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Game-related base classes\n",
        "'''\n",
        "class Card:\n",
        "    '''\n",
        "    Card stores the suit and rank of a single card\n",
        "\n",
        "    Note:\n",
        "        The suit variable in a standard card game should be one of [S, H, D, C, BJ, RJ] meaning [Spades, Hearts, Diamonds, Clubs, Black Joker, Red Joker]\n",
        "        Similarly the rank variable should be one of [A, 2, 3, 4, 5, 6, 7, 8, 9, T, J, Q, K]\n",
        "    '''\n",
        "    suit = None\n",
        "    rank = None\n",
        "    valid_suit = ['S', 'H', 'D', 'C', 'BJ', 'RJ']\n",
        "    valid_rank = ['-1','A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K']\n",
        "\n",
        "    def __init__(self, suit, rank):\n",
        "        ''' Initialize the suit and rank of a card\n",
        "\n",
        "        Args:\n",
        "            suit: string, suit of the card, should be one of valid_suit\n",
        "            rank: string, rank of the card, should be one of valid_rank\n",
        "        '''\n",
        "        self.suit = suit\n",
        "        self.rank = rank\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, Card):\n",
        "            return self.rank == other.rank\n",
        "        else:\n",
        "            # don't attempt to compare against unrelated types\n",
        "            return NotImplemented\n",
        "\n",
        "    def __hash__(self):\n",
        "        suit_index = Card.valid_suit.index(self.suit)\n",
        "        rank_index = Card.valid_rank.index(self.rank)\n",
        "        return rank_index + 100 * suit_index\n",
        "\n",
        "    def __str__(self):\n",
        "        ''' Get string representation of a card.\n",
        "\n",
        "        Returns:\n",
        "            string: the combination of rank and suit of a card. Eg: AS, 5H, JD, 3C, ...\n",
        "        '''\n",
        "        return self.rank\n",
        "\n",
        "    def get_index(self):\n",
        "        ''' Get index of a card.\n",
        "\n",
        "        Returns:\n",
        "            string: the combination of suit and rank of a card. Eg: 1S, 2H, AD, BJ, RJ...\n",
        "        '''\n",
        "        return self.suit+self.rank\n"
      ],
      "metadata": {
        "id": "TVB63Uhdgv35"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dealer and Judger"
      ],
      "metadata": {
        "id": "4E0M6GAZjCO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Dealer():\n",
        "\n",
        "    def __init__(self, seed = 1):\n",
        "        ''' Initialize a leducholdem dealer class\n",
        "        '''\n",
        "        np.random.seed(seed)\n",
        "        #self.np_random = np.random.RandomState()\n",
        "        self.deck = [\n",
        "                      Card('S', 'T'), Card('H', 'T'),Card('D', 'T'),Card('C', 'T'),\n",
        "                      Card('S', 'J'), Card('H', 'J'),Card('D', 'J'),Card('C', 'J'),\n",
        "                      Card('S', 'Q'), Card('H', 'Q'),Card('D', 'Q'),Card('C', 'Q'),\n",
        "                      Card('S', 'K'), Card('H', 'K'),Card('D', 'K'),Card('C', 'K'),\n",
        "                      Card('S', 'A'), Card('H', 'A'),Card('D', 'A'),Card('C', 'A')\n",
        "                      ]\n",
        "        self.shuffle()\n",
        "        self.pot = 0\n",
        "\n",
        "    def shuffle(self):\n",
        "        np.random.shuffle(self.deck)\n",
        "\n",
        "    def deal_card(self):\n",
        "        \"\"\"\n",
        "        Deal one card from the deck\n",
        "\n",
        "        Returns:\n",
        "            (Card): The drawn card from the deck\n",
        "        \"\"\"\n",
        "        return self.deck.pop()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    d = Dealer()\n",
        "    card_1=d.deal_card()\n",
        "    card_2 =d.deal_card()\n",
        "    print(card_1)\n",
        "    print(card_2)\n",
        "\n",
        "\n",
        "class Judger:\n",
        "    def __init__(self, num_of_active_players):\n",
        "        self.rewards = {\n",
        "            \"T\" : 10,\n",
        "            \"J\" : 11,\n",
        "            \"Q\" : 12,\n",
        "            \"K\" : 13,\n",
        "            \"A\" : 14\n",
        "        }\n",
        "        self.reward_per_player = [0]*num_of_active_players\n",
        "\n",
        "\n",
        "    def compare_hands(self, hands, table):\n",
        "        \"\"\"\n",
        "        hands: An array containing the cards that player i has been hanted\n",
        "        table: An array containing the cards on the table\n",
        "        \"\"\"\n",
        "        for player, hand in enumerate(hands): #for every hand\n",
        "\n",
        "            counts = table.count(hand)\n",
        "            counts = counts * 100 if counts == 2 else (counts * 10 if counts == 1 else counts)\n",
        "            self.reward_per_player[player] += (counts+1)* self.rewards.get(hand.rank)\n",
        "\n",
        "\n",
        "\n",
        "        return  -1 if self.reward_per_player[0] == self.reward_per_player[1] else np.argmax(self.reward_per_player)\n",
        "\n",
        "    def split_pot(self, pot, hands, table):\n",
        "\n",
        "        winner = self.compare_hands(hands, table)\n",
        "        if winner == -1:\n",
        "            return [pot/2, pot/2]\n",
        "        elif winner == 0:\n",
        "            return [pot, 0]\n",
        "        else:\n",
        "            return [0, pot]\n"
      ],
      "metadata": {
        "id": "G0Pskcshg0cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57eb8744-44cf-4297-d081-233f2cbc4283"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J\n",
            "Q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Game"
      ],
      "metadata": {
        "id": "Z8E-j4EqjLrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "INITIAL_TOKENS = 4.5\n",
        "ACTIONS = {\n",
        "    0:\"check\",\n",
        "    1:\"fold\",\n",
        "    2:\"raise\"\n",
        "}\n",
        "\n",
        "class Game:\n",
        "\n",
        "\n",
        "    def __init__(self,seed = 0, num_players=2, num_of_cards_in_hand=1, num_of_cards_on_table=2, num_of_rounds=2, done = False):\n",
        "        #logistics\n",
        "        self.seed = seed\n",
        "        np.random.seed(self.seed)\n",
        "        self.small_blind = 0.5\n",
        "        self.num_players = num_players\n",
        "        self.big_blind = self.small_blind\n",
        "        self.num_of_cards_in_hand = num_of_cards_in_hand\n",
        "        self.num_of_cards_on_table = num_of_cards_on_table\n",
        "        self.num_of_rounds = num_of_rounds\n",
        "        self.done = False #shows if the game is done and not the hand\n",
        "        self.mana = None\n",
        "        #info for every player\n",
        "        self.total_money_per_player = [INITIAL_TOKENS] * num_players\n",
        "        self.hand_of_player = [0]*num_players\n",
        "        self.active_players = [True] * num_players\n",
        "\n",
        "        #game state\n",
        "        self.current_round = 0 #how many steps have been played\n",
        "        self.opponent_last_action = None #the opponents last action before this betting round\n",
        "        self.current_phase = 0 #if we are in flop, or pre flop etc\n",
        "        self.consecutive_raises = 0\n",
        "        self.terminate_phase = 2 #shows how many bets we will have in just one phase (ex. the flop-phase etc)\n",
        "        self.table = [Card('S', '-1')]*2\n",
        "        self.last_bet= [0]*2\n",
        "    def init_game(self):\n",
        "        \"\"\"\n",
        "            Method that is called in the beginning of every game/hand of the \"tournament\"\n",
        "        \"\"\"\n",
        "\n",
        "        self.current_phase = 0\n",
        "        if self.done or self.check_if_game_end(): return -1#if at least one player is bancrupt the the tournament is over\n",
        "        self.current_round=0\n",
        "        s= np.random.randint(10_000)\n",
        "        self.dealer = Dealer(seed=s)\n",
        "        mana = np.random.choice([0,1])\n",
        "        #hand in the cards\n",
        "        for i in range(self.num_players):\n",
        "            self.hand_of_player[i] = self.dealer.deal_card()\n",
        "        self.total_money_per_player = list([i-self.small_blind for i in self.total_money_per_player]) #every player bets initially 0.5 tokens\n",
        "        self.pot = 2*self.small_blind\n",
        "        self.mana = mana\n",
        "        self.table = [Card('S', '-1')]*2\n",
        "        self.consecutive_raises = 0\n",
        "        self.last_bet= [0]*2\n",
        "        return mana\n",
        "\n",
        "    def step(self,action,player):\n",
        "\n",
        "        \"\"\"\n",
        "            it is called every time that a player talks,\n",
        "            returns True if the hand is over , else False\n",
        "        \"\"\"\n",
        "\n",
        "        self.current_round+=1\n",
        "        opponent = np.abs(player - 1)\n",
        "        #the only available option is \"fold\".You dont have the money to continue the game.\n",
        "        action = 1 if self.total_money_per_player[player] <1 and self.opponent_last_action == 2 else action #it was action = 1 ...\n",
        "        action = 0 if self.consecutive_raises == 2 and action == 2 else action\n",
        "\n",
        "        #original action has no meaning cause player has nothing to do\n",
        "\n",
        "\n",
        "        if action ==  1: #player folds\n",
        "            #the opponent wins\n",
        "            return self.win(player,opponent), action\n",
        "        if action == 2: #player raises\n",
        "\n",
        "            self.consecutive_raises +=1\n",
        "            if player != self.mana: self.terminate_phase = 3 #if player that talks second raise, then the opponent must answer in the same phase\n",
        "            if self.opponent_last_action == 2: #if opponent raised\n",
        "                if self.total_money_per_player[player] >= 2: #if I have the money I should bet 2 tokens\n",
        "                    self.pot +=2\n",
        "                    self.last_bet[player]= 2\n",
        "                    self.total_money_per_player[player]-=2\n",
        "                else: #else I lose\n",
        "\n",
        "                    return self.win(player,opponent), action\n",
        "\n",
        "            elif self.total_money_per_player[player] >= 1:# if opponent didn't raise and i have enough money\n",
        "                self.pot +=1\n",
        "                self.last_bet[player]= 2\n",
        "                self.total_money_per_player[player]-=1\n",
        "            else:# else you just lose\n",
        "                #so the opponent winds\n",
        "                return self.win(player,opponent), action\n",
        "        if action == 0:\n",
        "            if self.opponent_last_action ==2 : #if opponents raised in the last round\n",
        "                if self.total_money_per_player[player] >= 1:#(and has the money to do it)\n",
        "                    self.pot +=1\n",
        "                    self.total_money_per_player[player]-=1\n",
        "                else:#you dont have money to call\n",
        "                    return self.win(player,opponent), action\n",
        "\n",
        "        if self.terminate_phase <= self.current_round: #phase must terminate\n",
        "            if self.current_phase == 1: #at the end of the flop\n",
        "\n",
        "                judger = Judger(2)\n",
        "                r = judger.split_pot(self.pot,self.hand_of_player, self.table)\n",
        "\n",
        "                for i, reward in enumerate(r):\n",
        "                    self.total_money_per_player[i]+=reward\n",
        "                return True, action\n",
        "            else:\n",
        "                self.current_phase +=1\n",
        "                self.consecutive_raises = 0\n",
        "                self.opponent_last_action = 0\n",
        "                self.current_round = 0\n",
        "                self.terminate_phase = 2\n",
        "                #self.table = [self.dealer.deal_card(),self.dealer.deal_card()]\n",
        "                self.opponent_last_action = action\n",
        "                return False , action\n",
        "        self.opponent_last_action = action\n",
        "\n",
        "        return False, action\n",
        "\n",
        "    def all_in(self, player, opponent):\n",
        "\n",
        "        return\n",
        "    def win(self,player, opponent):\n",
        "        \"\"\" split the pot,\n",
        "            terminates the hand\n",
        "        \"\"\"\n",
        "\n",
        "        self.total_money_per_player[opponent]+=self.pot\n",
        "        self.pot = 0\n",
        "        self.done =  self.total_money_per_player[player] <= 0 or self.total_money_per_player[opponent] >= 9\n",
        "        self.opponent_last_action = None\n",
        "        return True\n",
        "\n",
        "\n",
        "    def check_if_game_end(self):\n",
        "        return  np.min(self.total_money_per_player) <.5 or np.max(self.total_money_per_player) > 2*INITIAL_TOKENS + .5\n",
        "\n"
      ],
      "metadata": {
        "id": "16Gw3uUYhOJI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils\n"
      ],
      "metadata": {
        "id": "9n3qyNcQjOno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#in case of Policy Iteratation\n",
        "'''\n",
        "0 : A pre flop     4 : K pre flop   8 : Q pre flop   12 : J pre flop  16 : 10 pre flop\n",
        "1 : A -A*          5 : K -K*        9 : Q - Q*       13 : J - J*      17 : 10 -10*\n",
        "2 : A - AA         6 : K-KK         10 : Q-QQ        14 : J - JJ      18 : 10 -10 10\n",
        "3 : A - **         7 : K - **       11 : Q - **      15 : J - **      19 : 10 - **\n",
        "\n",
        "---------------  ---------------  ---------------  ---------------  -------------------\n",
        "-----------actions----------\n",
        "0: check\n",
        "1: fold\n",
        "2: raise\n",
        "'''\n",
        "BEST_REWARD = 4.5\n",
        "WORST_REWARD = -BEST_REWARD\n",
        "MED_REWARD = BEST_REWARD/2\n",
        "LOW_MED_REWARD = BEST_REWARD/4\n",
        "LOW_BEST_REWARD = (3/4)*BEST_REWARD\n",
        "\n",
        "\n",
        "P = {\n",
        "    # A - pre flop\n",
        "   0: {\n",
        "        #action - check\n",
        "        0: [(0.2, 1, 0.0,False),\n",
        "            (0.1, 2, 0.0,False),\n",
        "            (0.7,3,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 0, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 1, 0.0,False),\n",
        "            (0.1, 2, 0.0,False),\n",
        "            (0.7, 3, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "     #A- A*\n",
        "    1: {\n",
        "        #check\n",
        "        0: [(0.5, 1, 0.0,False),\n",
        "            (0.5, 1, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 1, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 1, BEST_REWARD,True),\n",
        "            (0.5, 1, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "     #A-AA\n",
        "    2: {\n",
        "        #check\n",
        "        0: [(0.0, 2, 0.0,False),\n",
        "            (1, 2, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 2, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.0, 2, BEST_REWARD,True),\n",
        "            (1, 2, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    #A - **\n",
        "    3: {\n",
        "        #check\n",
        "        0: [(0.5, 3, 0.0,False),\n",
        "            (0.5, 3,LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 3, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 3, LOW_MED_REWARD,True), #only for A-** since A is the highest card\n",
        "            (0.5, 3, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    # K - pre flop\n",
        "   4: {\n",
        "        #check\n",
        "        0: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.7,7,0.0,False)\n",
        "        ],\n",
        "       #fold\n",
        "        1: [(1, 4, WORST_REWARD, True)\n",
        "        ],\n",
        "       #raise\n",
        "        2: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.7, 7, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "     #K- K*\n",
        "    5: {\n",
        "        #check\n",
        "        0: [(0.5, 5, 0.0,False),\n",
        "            (0.5, 5, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 5, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 5, BEST_REWARD,True),\n",
        "            (0.5, 5, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "     #K-KK\n",
        "    6: {\n",
        "        #check\n",
        "        0: [(0.5, 6, 0.0,False),\n",
        "            (0.5, 6, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 6, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 6, BEST_REWARD,True),\n",
        "            (0.5, 6, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    #K - **\n",
        "    7: {\n",
        "        #check\n",
        "        0: [(0.5, 7, 0.0,False),\n",
        "            (0.5, 7, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 7, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 7, LOW_MED_REWARD,True),\n",
        "            (0.5, 7, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    # Q - pre flop\n",
        "    8: {\n",
        "        #check\n",
        "        0: [(0.2,9, 0.0,False),\n",
        "            (0.1, 10, 0.0,False),\n",
        "            (0.7,11,0.0,False)\n",
        "        ],\n",
        "       #fold\n",
        "        1: [(1, 8, WORST_REWARD, True)\n",
        "        ],\n",
        "       #raise\n",
        "        2: [(0.2, 9, 0.0,False),\n",
        "            (0.1, 10, 0.0,False),\n",
        "            (0.7,11, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "     #Q- Q*\n",
        "    9: {\n",
        "        #check\n",
        "        0: [(0.5, 9, 0.0,False),\n",
        "            (0.5, 9,LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 9, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 9, BEST_REWARD,True),\n",
        "            (0.5, 9, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "     #Q-QQ\n",
        "    10: {\n",
        "        #check\n",
        "        0: [(0.5, 10, 0.0,False),\n",
        "            (0.5, 10, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 10, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 10, BEST_REWARD,True),\n",
        "            (0.5, 10, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    #Q - **\n",
        "    11: {\n",
        "        #check\n",
        "        0: [(0.5, 11, 0.0,False),\n",
        "            (0.5, 11, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 11, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 11, WORST_REWARD,True),\n",
        "            (0.5, 11, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    # J - pre flop\n",
        "   12: {\n",
        "        #check\n",
        "        0: [(0.2,13, 0.0,False),\n",
        "            (0.1, 14, 0.0,False),\n",
        "            (0.7,15,0.0,False)\n",
        "        ],\n",
        "       #fold\n",
        "        1: [(1, 12, WORST_REWARD, True)\n",
        "        ],\n",
        "       #raise\n",
        "        2: [(0.2,13, 0.0,False),\n",
        "            (0.1, 14, 0.0,False),\n",
        "            (0.7,15, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "     #J- J*\n",
        "    13: {\n",
        "        #check\n",
        "        0: [(0.5, 13, 0.0,False),\n",
        "            (0.5, 13, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 13, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 13, LOW_BEST_REWARD,True),\n",
        "            (0.5, 13, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "     #J-JJ\n",
        "    14: {\n",
        "        #check\n",
        "        0: [(0.5, 14, 0.0,False),\n",
        "            (0.5, 14, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 14, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 14, BEST_REWARD,True),\n",
        "            (0.5, 14, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    #J - **\n",
        "    15: {\n",
        "        #check\n",
        "        0: [(0.5, 15, 0.0,False),\n",
        "            (0.5, 15, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 15, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 15, WORST_REWARD,True),\n",
        "            (0.5, 15, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    # 10 - pre flop\n",
        "   16: {\n",
        "        #action - check\n",
        "        0: [(0.2, 17, 0.0,False),\n",
        "            (0.1, 18, 0.0,False),\n",
        "            (0.7,19,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 16, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 17, 0.0,False),\n",
        "            (0.1, 18, 0.0,False),\n",
        "            (0.7, 19, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "     #10-10*\n",
        "    17: {\n",
        "        #check\n",
        "        0: [(0.5, 17, 0.0,False),\n",
        "            (0.5, 17, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 17, MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 17, WORST_REWARD,True),\n",
        "            (0.5, 17, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "     #10-1010\n",
        "    18: {\n",
        "        #check\n",
        "        0: [(0.5, 18, 0.0,False),\n",
        "            (0.5, 18, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 18, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 18, BEST_REWARD,True),\n",
        "            (0.5, 18, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "    #10-**\n",
        "    19: {\n",
        "        #check\n",
        "        0: [(0.5, 19, 0.0,False),\n",
        "            (0.5, 19, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 19, LOW_BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 19, WORST_REWARD,True),\n",
        "            (0.5, 19, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "}\n",
        "\n",
        "'''\n",
        "The order of the each tuple is [card in hand, card on the table, phase, opponent last action]\n",
        "\n",
        "\n",
        "---------------  ---------------  ---------------  ---------------  -------------------\n",
        "\n",
        "\n",
        "0: A-AA or A-A*                     9: Q pre flop raise                18:J-J*, flop, raise         27:T-T*, flop, check\n",
        "1:A- ** , flop, raise               10: Q pre flop check , no info     19:J-J*, flop, check         28:T-TT, flop, raise\n",
        "2: A- **, flop, check               11: Q-Q* ,flop, raise              20:J -JJ, flop, raise        29:T- TT, flop, check\n",
        "3:K pre flop raise                  12: Q-Q*, flop, check -na kn raise 21:J - JJ, flop, check       30:T- ** , flop, raise\n",
        "4: K pre flop, check or no info     13: Q-QQ , flop raise or check     22:J- ** , flop, raise\t    31:T- **, flop, check\n",
        "5: K-K* flop, raise or check        14: Q- ** , flop, raise            23:J- **, flop, check\t    32:A- pre flop any opp action\n",
        "6: K-KK flop, raise or check        15: Q- **, flop, check             24:T pre flop, raise\n",
        "7:  K-** flop, raise                16: J pre flop, raise              25: T pre flop, check/no info\n",
        "8 : K-** flop, check                17: J pre flop, check - no info    26:T -T*,  flop, raise\n",
        "\n",
        "\n",
        "-----------actions----------\n",
        "0: check\n",
        "1: fold\n",
        "2: raise\n",
        "'''\n",
        "\n",
        "P_THRESHOLD_D= {\n",
        "    # A-AA or A-A* whatever the opp does, raise\n",
        "   0: {\n",
        "        #action - check\n",
        "        0: [(0.5, 0, 0.0,False),\n",
        "            (0.5, 0, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 0, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 0, BEST_REWARD,True),\n",
        "            (0.5, 0, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "   1: { #A- ** , flop, raise  if the opp raised, he has sth. Better fold.\n",
        "        #action - check\n",
        "        0: [(0.5, 1, 0.0,False),\n",
        "            (0.5, 1, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 1, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 1, WORST_REWARD,True),\n",
        "            (0.5, 1, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "   2: { #A- **, flop, check : The opponent doesnt have sth good. Raise.\n",
        "        #action - check\n",
        "        0: [(0.5, 2, 0.0,False),\n",
        "            (0.5, 2, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 2, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 2, BEST_REWARD,True),\n",
        "            (0.5, 2, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "# K- pre flop, raise : low best is to raise\n",
        "   3: {\n",
        "        #action - check\n",
        "        0: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35,8,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 3, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35, 8, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#K pre flop, check or no info\n",
        "   4: {\n",
        "        #action - check\n",
        "        0: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35,8,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 3, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35, 8, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#5: K-K* flop, raise or check : we probably have the winning card, lets raise\n",
        "   5: {\n",
        "        #action - check\n",
        "        0: [(0.5, 5, 0.0,False),\n",
        "            (0.5, 5, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 5, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 5, LOW_MED_REWARD,True),\n",
        "            (0.5, 5, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#6: K-KK flop, raise or check : we have the winning hand unless the opp has A-AA -- best to raise\n",
        "   6: {\n",
        "        #action - check\n",
        "        0: [(0.5, 6, 0.0,False),\n",
        "            (0.5, 6, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 6, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 6, BEST_REWARD,True),\n",
        "            (0.5, 6, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#7:  K-** flop, raise -- the other has sth, best is check otherwise fold\n",
        "   7: {\n",
        "        #action - check\n",
        "        0: [(0.5, 7, 0.0,False),\n",
        "            (0.5, 7, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 7, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 7, WORST_REWARD,True),\n",
        "            (0.5, 7, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#8 : K-** flop, check -- they dont have anything, ur probably bettterrrr-- low best is raise\n",
        "    8: {\n",
        "        #action - check\n",
        "        0: [(0.5, 8, 0.0,False),\n",
        "            (0.5, 8, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 8, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 8, LOW_BEST_REWARD,True),\n",
        "            (0.5, 8, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#9: Q pre flop raise\n",
        "   9: {\n",
        "        #action - check\n",
        "        0: [(0.1, 11, 0.0,False),\n",
        "            (0.1, 12, 0.0,False),\n",
        "            (0.1, 13, 0.0,False),\n",
        "            (0.35,14, 0.0,False),\n",
        "            (0.35,15, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 9,MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 11, 0.0,False),\n",
        "            (0.1, 12, 0.0,False),\n",
        "            (0.1, 13, 0.0,False),\n",
        "            (0.35,14, 0.0,False),\n",
        "            (0.35,15, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#10: Q pre flop check , no info\n",
        "    10: {\n",
        "    #action - check\n",
        "    0: [(0.1, 11, 0.0,False),\n",
        "        (0.1, 12, 0.0,False),\n",
        "        (0.1, 13, 0.0,False),\n",
        "        (0.35,14, 0.0,False),\n",
        "        (0.35,15, 0.0,False)\n",
        "    ],\n",
        "    #action -fold\n",
        "    1: [(1, 9, WORST_REWARD, True)\n",
        "    ],\n",
        "    #action -raise\n",
        "    2: [(0.1, 11, 0.0,False),\n",
        "        (0.1, 12, 0.0,False),\n",
        "        (0.1, 13, 0.0,False),\n",
        "        (0.35,14, 0.0,False),\n",
        "        (0.35,15, 0.0,False)\n",
        "    ]\n",
        "},\n",
        "#11: Q-Q* ,flop, raise\n",
        "   11: {\n",
        "        #action - check\n",
        "        0: [(0.5, 11, 0.0,False),\n",
        "            (0.5, 11, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 11, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 11, MED_REWARD,True),\n",
        "            (0.5, 11, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#12: Q-Q*, flop, check  --the opp has nothing , raise!\n",
        "    12: {\n",
        "        #action - check\n",
        "        0: [(0.5, 12, 0.0,False),\n",
        "            (0.5, 12, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 12, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 12, BEST_REWARD,True),\n",
        "            (0.5, 12, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#13: Q-QQ , flop raise or check\n",
        "   13: {\n",
        "        #action - check\n",
        "        0: [(0.5, 13, 0.0,False),\n",
        "            (0.5, 13, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 13, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 13, BEST_REWARD,True),\n",
        "            (0.5, 13, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#14: Q- ** , flop, raise -- we have nothing , they have sth--fold\n",
        "   14: {\n",
        "        #action - check\n",
        "        0: [(0.5, 14, 0.0,False),\n",
        "            (0.5, 14, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 14, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 14, WORST_REWARD,True),\n",
        "            (0.5, 14, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#15: Q- **, flop, check --you both have nothing its goona be highest card winning\n",
        "     15: {\n",
        "        #action - check\n",
        "        0: [(0.5, 8, 0.0,False),\n",
        "            (0.5, 8, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 8, LOW_BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 8, BEST_REWARD,True),\n",
        "            (0.5, 8, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#16: J pre flop, raise--fold\n",
        "   16: {\n",
        "        #action - check\n",
        "        0: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 16, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)]\n",
        "    },\n",
        "#17: J pre flop, check - no info\n",
        "   17: {\n",
        "        #action - check\n",
        "        0: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 17, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)]\n",
        "    },\n",
        "\n",
        "#18:J-J*, flop, raise  --probs has sth better,still better not fold\n",
        "   18: {\n",
        "        #action - check\n",
        "        0: [(0.5, 18, 0.0,False),\n",
        "            (0.5, 18, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 18, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 18, WORST_REWARD,True),\n",
        "            (0.5, 18, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#19: J-J*, flop, check --they have nothing. I win probs, raise\n",
        "   19: {\n",
        "        #action - check\n",
        "        0: [(0.5, 19, 0.0,False),\n",
        "            (0.5, 19, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 19, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 19, BEST_REWARD,True),\n",
        "            (0.5, 19, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#20:J -JJ, flop, raise\n",
        "   20: {\n",
        "        #action - check\n",
        "        0: [(0.5, 20, 0.0,False),\n",
        "            (0.5, 20, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 20, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 20, LOW_BEST_REWARD,True),\n",
        "            (0.5, 20, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#21:J - JJ, flop, check -- i win. raise only.\n",
        "   21: {\n",
        "        #action - check\n",
        "        0: [(0.5, 21, 0.0,False),\n",
        "            (0.5, 21, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 21, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 21, BEST_REWARD,True),\n",
        "            (0.5, 21, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "#22:J- ** , flop, raise --fold asap\n",
        "   22: {\n",
        "        #action - check\n",
        "        0: [(0.5, 22, 0.0,False),\n",
        "            (0.5, 22, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 22, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 22, WORST_REWARD,True),\n",
        "            (0.5, 22, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "#23:J- **, flop, check\n",
        "   23: {\n",
        "        #action - check\n",
        "        0: [(0.5, 23, 0.0,False),\n",
        "            (0.5, 23, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 23, LOW_BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 23, WORST_REWARD,True),\n",
        "            (0.5, 23, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#24:T pre flop, raise\n",
        "   24: {\n",
        "        #action - check\n",
        "        0: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 24, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)]\n",
        "    },\n",
        "#25: T pre flop, check/no info\n",
        "   25: {\n",
        "        #action - check\n",
        "        0: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 24, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)]\n",
        "    },\n",
        "#26: T -T*,  flop, raise\n",
        "   26: {\n",
        "        #action - check\n",
        "        0: [(0.5, 26, 0.0,False),\n",
        "            (0.5, 26, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 26, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 26, LOW_MED_REWARD,True),\n",
        "            (0.5, 26, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#27:T-T*, flop, check - im winning\n",
        "   27: {\n",
        "        #action - check\n",
        "        0: [(0.5, 27, 0.0,False),\n",
        "            (0.5, 27, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 27, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 27, BEST_REWARD,True),\n",
        "            (0.5, 27, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#28:T-TT, flop, raise\n",
        "   28: {\n",
        "        #action - check\n",
        "        0: [(0.5, 28, 0.0,False),\n",
        "            (0.5, 28, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 28, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 28, LOW_BEST_REWARD,True),\n",
        "            (0.5, 28, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#29:T- TT, flop, check - i win for sure.\n",
        "   29: {\n",
        "        #action - check\n",
        "        0: [(0.5, 29, 0.0,False),\n",
        "            (0.5, 29, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 29, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 29, BEST_REWARD,True),\n",
        "            (0.5, 29, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#30 :T- ** , flop, raise --fold.\n",
        "   30: {\n",
        "        #action - check\n",
        "        0: [(0.5, 30, 0.0,False),\n",
        "            (0.5, 30, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 30, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 30, WORST_REWARD,True),\n",
        "            (0.5, 30, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#31: T- **, flop, check\n",
        "   31: {\n",
        "        #action - check\n",
        "        0: [(0.5, 31, 0.0,False),\n",
        "            (0.5, 31, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 31, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 31, WORST_REWARD,True),\n",
        "            (0.5, 31, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#32: A- pre flop any opp action -- better raise\n",
        "   32: {\n",
        "        #action - check\n",
        "        0: [(0.3, 0, 0.0,False),\n",
        "            (0.35, 1, 0.0,False),\n",
        "            (0.35, 2, 0.0,False)\n",
        "\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 32, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.3, 0, 0.0,False),\n",
        "            (0.35, 1, 0.0,False),\n",
        "            (0.35, 2, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "P_THRESHOLD_A= {\n",
        "   # A-AA or A-A* whatever the opp does, raise\n",
        "    0: {\n",
        "        #action - check\n",
        "        0: [(0.5, 0, 0.0,False),\n",
        "            (0.5, 0, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 0, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 0, BEST_REWARD,True),\n",
        "            (0.5, 0, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "    1: { #A- ** , flop, raise  if the opp raised, he has sth. Better fold.\n",
        "        #action - check\n",
        "        0: [(0.5, 1, 0.0,False),\n",
        "            (0.5, 1, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 1, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 1, WORST_REWARD,True),\n",
        "            (0.5, 1, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "   2: { #A- **, flop, check : The opponent doesnt have sth good. Raise.\n",
        "        #action - check\n",
        "        0: [(0.5, 2, 0.0,False),\n",
        "            (0.5, 2, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 2, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 2, BEST_REWARD,True),\n",
        "            (0.5, 2, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "# K- pre flop, raise : low best is to raise\n",
        "   3: {\n",
        "        #action - check\n",
        "        0: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35,8,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 3, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35, 8, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#K pre flop, check or no info\n",
        "   4: {\n",
        "        #action - check\n",
        "        0: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35,8,0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 3, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.2, 5, 0.0,False),\n",
        "            (0.1, 6, 0.0,False),\n",
        "            (0.35, 7, 0.0,False),\n",
        "            (0.35, 8, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#5: K-K* flop, raise or check : we probably have the winning card, lets raise\n",
        "   5: {\n",
        "        #action - check\n",
        "        0: [(0.5, 5, 0.0,False),\n",
        "            (0.5, 5, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 5, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 5, BEST_REWARD,True),\n",
        "            (0.5, 5, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#6: K-KK flop, raise or check : we have the winning hand unless the opp has A-AA -- best to raise\n",
        "   6: {\n",
        "        #action - check\n",
        "        0: [(0.5, 6, 0.0,False),\n",
        "            (0.5, 6, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 6, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 6, BEST_REWARD,True),\n",
        "            (0.5, 6, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#7:  K-** flop, raise -- the other has sth, best is check otherwise fold\n",
        "   7: {\n",
        "        #action - check\n",
        "        0: [(0.5, 7, 0.0,False),\n",
        "            (0.5, 7, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 7, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 7, WORST_REWARD,True),\n",
        "            (0.5, 7, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#8 : K-** flop, check -- they dont have anything, ur probably bettterrrr-- low best is raise\n",
        "    8: {\n",
        "        #action - check\n",
        "        0: [(0.5, 8, 0.0,False),\n",
        "            (0.5, 8, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 8, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 8, LOW_BEST_REWARD,True),\n",
        "            (0.5, 8, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#9: Q pre flop raise\n",
        "   9: {\n",
        "        #action - check\n",
        "        0: [(0.1, 11, 0.0,False),\n",
        "            (0.1, 12, 0.0,False),\n",
        "            (0.1, 13, 0.0,False),\n",
        "            (0.35,14, 0.0,False),\n",
        "            (0.35,15, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 9,MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 11, 0.0,False),\n",
        "            (0.1, 12, 0.0,False),\n",
        "            (0.1, 13, 0.0,False),\n",
        "            (0.35,14, 0.0,False),\n",
        "            (0.35,15, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#10: Q pre flop check , no info\n",
        "    10: {\n",
        "    #action - check\n",
        "    0: [(0.1, 11, 0.0,False),\n",
        "        (0.1, 12, 0.0,False),\n",
        "        (0.1, 13, 0.0,False),\n",
        "        (0.35,14, 0.0,False),\n",
        "        (0.35,15, 0.0,False)\n",
        "    ],\n",
        "    #action -fold\n",
        "    1: [(1, 9, WORST_REWARD, True)\n",
        "    ],\n",
        "    #action -raise\n",
        "    2: [(0.1, 11, 0.0,False),\n",
        "        (0.1, 12, 0.0,False),\n",
        "        (0.1, 13, 0.0,False),\n",
        "        (0.35,14, 0.0,False),\n",
        "        (0.35,15, 0.0,False)\n",
        "    ]\n",
        "},\n",
        "#11: Q-Q* ,flop, raise\n",
        "   11: {\n",
        "        #action - check\n",
        "        0: [(0.5, 11, 0.0,False),\n",
        "            (0.5, 11, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 11, MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 11, BEST_REWARD,True),\n",
        "            (0.5, 11, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#12: Q-Q*, flop, check  --\n",
        "    12: {\n",
        "        #action - check\n",
        "        0: [(0.5, 12, 0.0,False),\n",
        "            (0.5, 12, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 12, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 12, BEST_REWARD,True),\n",
        "            (0.5, 12, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#13: Q-QQ , flop raise or check\n",
        "   13: {\n",
        "        #action - check\n",
        "        0: [(0.5, 13, 0.0,False),\n",
        "            (0.5, 13, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 13, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 13, BEST_REWARD,True),\n",
        "            (0.5, 13, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#14: Q- ** , flop, raise -- we have nothing , they have sth--fold\n",
        "   14: {\n",
        "        #action - check\n",
        "        0: [(0.5, 14, 0.0,False),\n",
        "            (0.5, 14, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 14, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 14, WORST_REWARD,True),\n",
        "            (0.5, 14, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#15: Q- **, flop, check --you both have nothing its goona be highest card winning\n",
        "     15: {\n",
        "        #action - check\n",
        "        0: [(0.5, 8, 0.0,False),\n",
        "            (0.5, 8, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 8, LOW_BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 8, BEST_REWARD,True),\n",
        "            (0.5, 8, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#16: J pre flop, raise--fold\n",
        "   16: {\n",
        "        #action - check\n",
        "        0: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 16, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)]\n",
        "    },\n",
        "#17: J pre flop, check - no info\n",
        "   17: {\n",
        "        #action - check\n",
        "        0: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 17, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 18, 0.0,False),\n",
        "            (0.1, 19, 0.0,False),\n",
        "            (0.05, 20, 0.0,False),\n",
        "            (0.05,21, 0.0,False),\n",
        "            (0.35,22, 0.0,False),\n",
        "            (0.35,23, 0.0,False)]\n",
        "    },\n",
        "\n",
        "#18:J-J*, flop, raise  --probs has sth better,still better not fold\n",
        "   18: {\n",
        "        #action - check\n",
        "        0: [(0.5, 18, 0.0,False),\n",
        "            (0.5, 18, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 18, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 18, WORST_REWARD,True),\n",
        "            (0.5, 18, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#19: J-J*, flop, check --they have nothing. I win probs, raise\n",
        "   19: {\n",
        "        #action - check\n",
        "        0: [(0.5, 19, 0.0,False),\n",
        "            (0.5, 19, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 19, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 19, BEST_REWARD,True),\n",
        "            (0.5, 19, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#20:J -JJ, flop, raise\n",
        "   20: {\n",
        "        #action - check\n",
        "        0: [(0.5, 20, 0.0,False),\n",
        "            (0.5, 20, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 20, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 20, LOW_BEST_REWARD,True),\n",
        "            (0.5, 20, 0.0,False)\n",
        "        ]\n",
        "    },\n",
        "#21:J - JJ, flop, check -- i win. raise only.\n",
        "   21: {\n",
        "        #action - check\n",
        "        0: [(0.5, 21, 0.0,False),\n",
        "            (0.5, 21, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 21, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 21, BEST_REWARD,True),\n",
        "            (0.5, 21, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "#22:J- ** , flop, raise --fold asap\n",
        "   22: {\n",
        "        #action - check\n",
        "        0: [(0.5, 22, 0.0,False),\n",
        "            (0.5, 22, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 22, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 22, WORST_REWARD,True),\n",
        "            (0.5, 22, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "#23:J- **, flop, check\n",
        "   23: {\n",
        "        #action - check\n",
        "        0: [(0.5, 23, 0.0,False),\n",
        "            (0.5, 23, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 23, LOW_BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 23, WORST_REWARD,True),\n",
        "            (0.5, 23, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#24:T pre flop, raise\n",
        "   24: {\n",
        "        #action - check\n",
        "        0: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 24, LOW_MED_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)]\n",
        "    },\n",
        "#25: T pre flop, check/no info\n",
        "   25: {\n",
        "        #action - check\n",
        "        0: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 24, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.1, 26, 0.0,False),\n",
        "            (0.1, 27, 0.0,False),\n",
        "            (0.05, 28, 0.0,False),\n",
        "            (0.05,29, 0.0,False),\n",
        "            (0.35,30, 0.0,False),\n",
        "            (0.35,31, 0.0,False)]\n",
        "    },\n",
        "#26: T -T*,  flop, raise\n",
        "   26: {\n",
        "        #action - check\n",
        "        0: [(0.5, 26, 0.0,False),\n",
        "            (0.5, 26, LOW_BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 26, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 26, LOW_MED_REWARD,True),\n",
        "            (0.5, 26, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#27:T-T*, flop, check - im winning\n",
        "   27: {\n",
        "        #action - check\n",
        "        0: [(0.5, 27, 0.0,False),\n",
        "            (0.5, 27, LOW_MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 27, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 27, BEST_REWARD,True),\n",
        "            (0.5, 27, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#28:T-TT, flop, raise\n",
        "   28: {\n",
        "        #action - check\n",
        "        0: [(0.5, 28, 0.0,False),\n",
        "            (0.5, 28, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 28, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 28, BEST_REWARD,True),\n",
        "            (0.5, 28, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#29:T- TT, flop, check - i win for sure.\n",
        "   29: {\n",
        "        #action - check\n",
        "        0: [(0.5, 29, 0.0,False),\n",
        "            (0.5, 29, WORST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 29, WORST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 29, BEST_REWARD,True),\n",
        "            (0.5, 29, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#30 :T- ** , flop, raise --fold.\n",
        "   30: {\n",
        "        #action - check\n",
        "        0: [(0.5, 30, 0.0,False),\n",
        "            (0.5, 30, MED_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 30, BEST_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 30, WORST_REWARD,True),\n",
        "            (0.5, 30, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#31: T- **, flop, check\n",
        "   31: {\n",
        "        #action - check\n",
        "        0: [(0.5, 31, 0.0,False),\n",
        "            (0.5, 31, BEST_REWARD,True),\n",
        "\n",
        "        ],\n",
        "        #fold\n",
        "        1: [(1, 31, LOW_MED_REWARD,True)\n",
        "        ],\n",
        "        #raise\n",
        "        2: [(0.5, 31, WORST_REWARD,True),\n",
        "            (0.5, 31, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "#32: A- pre flop any opp action -- better raise\n",
        "   32: {\n",
        "        #action - check\n",
        "        0: [(0.3, 0, 0.0,False),\n",
        "            (0.35, 1, 0.0,False),\n",
        "            (0.35, 2, 0.0,False)\n",
        "\n",
        "        ],\n",
        "       #action -fold\n",
        "        1: [(1, 32, WORST_REWARD, True)\n",
        "        ],\n",
        "       #action -raise\n",
        "        2: [(0.3, 0, 0.0,False),\n",
        "            (0.35, 1, 0.0,False),\n",
        "            (0.35, 2, 0.0,False)\n",
        "\n",
        "        ]\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_pre_flop_state_to_num(state):\n",
        "    card = np.where(np.array(state) == 1)\n",
        "    state = (4-card[0][0])*4\n",
        "    return state\n",
        "\n",
        "def convert_flop_state_to_num(preflop_state,state):\n",
        "    \"\"\" state is the full state (hand+table) =an array of 10 numbers/5 for preflop 5 for flop cards\"\"\"\n",
        "    indices = np.where(np.array(state) == 1)[0]\n",
        "    if(len(indices)==1): return preflop_state #if we find one 1 then we re at the preflop state\n",
        "    if(len(indices) == 2): #we have a pair on the table, otherwise it would be 3\n",
        "        if indices[0] + 5 == indices[1]: #if the pair is the same rank as the preflop\n",
        "            return preflop_state + 2\n",
        "        return preflop_state +3 # e.g. A-**\n",
        "    if ((indices[0]+5 == indices[1] or indices[0]+ 5 == indices[2] )and(len(indices) == 3)): return preflop_state + 1\n",
        "    return preflop_state +3\n",
        "\n",
        "def threshold_convert_state_to_num(state):\n",
        "   # Map the card in hand to an index\n",
        "   hand_card = np.where(np.array(state[0:5]) == 1)[0]\n",
        "\n",
        "   # Map the card on the table to an index\n",
        "   table_card = np.where(np.array(state[5:10]) == 1)[0]\n",
        "\n",
        "   # Map the opponent's last action to an index\n",
        "   opponent_action =  state[10:]\n",
        "   if same_list(opponent_action, [0,0,0]): opponent_action[:]= [1,0,0] #same as check\n",
        "##################pre flop phase############################\n",
        "   if(len(table_card)==0): #we re at the pre-flop states\n",
        "     #24:T pre flop, raise\n",
        "     #25: T pre flop, check/no info\n",
        "        if (state[0] == 1):\n",
        "            if same_list(opponent_action, [0,0,0]) : return 25 #no info\n",
        "            if same_list(opponent_action, [1,0,0]) : return 25 #check\n",
        "            if same_list(opponent_action, [0,0,1]) : return 24 #raise\n",
        "\n",
        "        #16: J pre flop, raise\n",
        "        #17: J pre flop, check - no info\n",
        "        if(state[1] ==1):\n",
        "            if same_list(opponent_action, [0,0,0]) : return 17 #no info\n",
        "            if same_list(opponent_action, [1,0,0]) : return 17 #check\n",
        "            if same_list(opponent_action, [0,0,1]) : return 16 #raise\n",
        "        #9: Q pre flop raise\n",
        "        #10: Q pre flop check , no info\n",
        "        if(state[2]==1):\n",
        "            if same_list(opponent_action, [0,0,0]) : return 10 #no info\n",
        "            if same_list(opponent_action, [1,0,0]) : return 10 #check\n",
        "            if same_list(opponent_action, [0,0,1]) : return 9 #raise\n",
        "       #3:K pre flop raise\n",
        "       #4: K pre flop, check or no info\n",
        "        if(state[3]==1):\n",
        "            if same_list(opponent_action, [0,0,0]) : return 4 #no info\n",
        "            if same_list(opponent_action, [1,0,0]) : return 4 #check\n",
        "            if same_list(opponent_action, [0,0,1]) : return 3 #raise\n",
        "        #32:A- pre flop any opp action\n",
        "        if(state[4]==1): return 32\n",
        "\n",
        "###########################flop phase###############################\n",
        "   if(len(table_card)==2): #we have 2 different (between them) cards on the table\n",
        "        #30:T- ** , flop, raise\n",
        "        #31:T- **, flop, check\n",
        "        if (state[0] == 1 and state[5] == 0):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 31 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 30 #raise\n",
        "\n",
        "        #26:T -T*,  flop, raise\n",
        "        #27:T-T*, flop, check\n",
        "        if(state[0] == 1 and state[5] == 1):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 27 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 26 #raise\n",
        "\n",
        "        #22:J- ** , flop, raise\n",
        "        #23:J- **, flop, check\n",
        "        if(state[1] ==1 and state[6]==0):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 23 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 22 #raise\n",
        "        #18:J-J*, flop, raise\n",
        "        #19:J-J*, flop, check\n",
        "        if (state[1] ==1 and state[6]==1):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 19 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 18 #raise\n",
        "        #14: Q- ** , flop, raise\n",
        "        #15: Q- **, flop, check\n",
        "        if(state[2]==1 and state[7]==0):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 15 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 14 #raise\n",
        "            return 15\n",
        "        #11: Q-Q* ,flop, raise\n",
        "        #12: Q-Q*, flop, check\n",
        "        if(state[2]==1 and state[7]==1):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 12 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 11 #raise\n",
        "        #7:  K-** flop, raise\n",
        "        #8 : K-** flop, check\n",
        "        if(state[3]==1 and state[8]==0):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 8 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 7 #raise\n",
        "        #5: K-K* flop, raise or check\n",
        "        if(state[3]==1 and state[8]==1):\n",
        "            return 5\n",
        "        #1:A- ** , flop, raise\n",
        "        #2: A- **, flop, check\n",
        "        if(state[4]==1 and state[9]==0):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 2 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 1 #raise\n",
        "        #0: A-AA or A-A*\n",
        "        if(state[4]==1 and state[9]==1):\n",
        "            return 0\n",
        "\n",
        "#######################pair on the table already ##########################\n",
        "   if(len(table_card)==1): #we have a pair already on the table\n",
        "      #28:T-TT, flop, raise\n",
        "      #29:T- TT, flop, check\n",
        "        if(state[0] == 1 and state[5] == 1):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 29 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 28 #raise\n",
        "        if (state[0] == 1 and state[5] ==0): #there is a pair in the table but we cannot make a 3 of a kind\n",
        "            #act same as T-**\n",
        "            if same_list(opponent_action,[1,0,0]) : return 31\n",
        "            if same_list(opponent_action,[0,0,1]) : return 30\n",
        "        #20:J -JJ, flop, raise\n",
        "        #21:J - JJ, flop, check\n",
        "        if (state[1] ==1 and state[6]==1):\n",
        "            if same_list(opponent_action,[1,0,0]) : return 21 #check\n",
        "            if same_list(opponent_action,[0,0,1]) : return 20 #raise\n",
        "        if (state[1] == 1 and state[6] ==0): #there is a pair in the table but we cannot make a 3 of a kind\n",
        "            #act same as J-**\n",
        "            if same_list(opponent_action,[1,0,0]) : return 23\n",
        "            if same_list(opponent_action,[0,0,1]) : return 22\n",
        "        #13: Q-QQ , flop raise or check\n",
        "        if(state[2]==1 and state[7]==1):return 13\n",
        "        if(state[2]==1 and state[7] ==0):\n",
        "            #act same as Q-**\n",
        "            if same_list(opponent_action,[1,0,0]) : return 12\n",
        "            if same_list(opponent_action,[0,0,1]) : return 11\n",
        "        #6: K-KK flop, raise or check\n",
        "        if(state[3]==1 and state[8]==1):return 6\n",
        "        if (state[3] == 1 and state[8] ==0): #there is a pair in the table but we cannot make a 3 of a kind\n",
        "            #act same as K-**\n",
        "            if same_list(opponent_action,[1,0,0]) : return 8\n",
        "            if same_list(opponent_action,[0,0,1]) : return 7\n",
        "        #0: A-AA or A-A*\n",
        "        if(state[4]==1 and state[9]==1):return 0\n",
        "        if (state[4] == 1 and state[9] ==0): #there is a pair in the table but we cannot make a 3 of a kind\n",
        "            #act same as A-**\n",
        "            if same_list(opponent_action,[1,0,0]) : return 2\n",
        "            if same_list(opponent_action,[0,0,1]) : return 1\n",
        "\n",
        "   return state\n",
        "\n",
        "\n",
        "def same_list(first, second):\n",
        "    if len(first)!=len(second):\n",
        "        return False\n",
        "    same = True\n",
        "    for i, j in zip(first, second):\n",
        "        same = i==j\n",
        "        if  not same:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def return_state(state_vector, threshold, agent, preflop_state, full_stages=False):\n",
        "    state = 0\n",
        "\n",
        "    if (agent.to_str() == \"Q_Learning_Agent\" and full_stages):\n",
        "        state_vector = state_vector[0:10] #if not threshold else state_vector\n",
        "        state_vector = state_vector[::-1]\n",
        "        return int(\"\".join(map(str, state_vector)), 2)\n",
        "\n",
        "    if not threshold:\n",
        "        state_vector = state_vector[0:10]\n",
        "        return convert_flop_state_to_num(preflop_state, state_vector)\n",
        "    else :\n",
        "        return threshold_convert_state_to_num(state_vector)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def differ(first, second):\n",
        "    count = 0\n",
        "    for i, j in zip(first, second):\n",
        "        if (i != j): count+=1\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "id": "EmYRdqzljyCF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment"
      ],
      "metadata": {
        "id": "5F65OTHzkCAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Env:\n",
        "\n",
        "    def __init__(self,agent, opponent, seed = 0, number_of_cards=5):\n",
        "        self.game = Game(seed = seed)\n",
        "        self.agent = agent\n",
        "        self.agents_hand = [0]*number_of_cards\n",
        "        self.agents_chips = []\n",
        "        self.chip_index = 0\n",
        "        self.opponent = opponent\n",
        "        self.opponents_hand = []\n",
        "        self.table = [Card('S', '-1')]*2\n",
        "        #chips of the agent\n",
        "        total_chips = int(INITIAL_TOKENS * 4 + 1)\n",
        "        self.agents_chips = [0]*total_chips\n",
        "        self.total_chips = total_chips\n",
        "        self.chip_index = int((total_chips - 1)/2)\n",
        "        self.agents_chips[self.chip_index] = 1\n",
        "        #back account in the begining of every hand\n",
        "        self.bank = []\n",
        "        #storing the opponents move in order to return it on state\n",
        "        self.last_opponent_move = -1\n",
        "\n",
        "    def reset(self, disp = False):\n",
        "\n",
        "\n",
        "        self.mana = self.game.init_game()\n",
        "        if self.mana == -1 : return [-1] * 3\n",
        "\n",
        "        agents_hand = self.game.hand_of_player[0] #by assumption, trainable agent is player 0 (doesn't mean that plays first every time)\n",
        "        self.agents_hand = [\"T\", \"J\", \"Q\", \"K\", \"A\"] #important line in order to create the correct state vector\n",
        "        self.agents_hand = list([1 if agents_hand.rank == i else 0 for i in self.agents_hand])\n",
        "        self.opponents_hand = self.game.hand_of_player[1]\n",
        "        #setting up the ante\n",
        "        self.bank = list([ i+self.game.small_blind for i in self.game.total_money_per_player]) #blind/ante, +self.game.small_blind is right\n",
        "        if self.game.small_blind !=0:\n",
        "            if self.chip_index == 0:return [-1] * 3\n",
        "            self.agents_chips[self.chip_index] = 0\n",
        "            self.agents_chips[self.chip_index-1] = 1\n",
        "            self.chip_index-=1\n",
        "\n",
        "        self.last_opponent_move = None\n",
        "        done = False\n",
        "        self.table = [Card('S', '-1')]*2\n",
        "        state = self.form_state()\n",
        "        self.disp = disp\n",
        "        return state, self.mana, done\n",
        "\n",
        "    def calulate_chips(self):\n",
        "        bank = self.game.total_money_per_player\n",
        "        self.chip_index = int(bank[0]*2)\n",
        "        self.agents_chips=[0]*self.total_chips\n",
        "        self.agents_chips[self.chip_index] = 1\n",
        "        return\n",
        "\n",
        "    def step(self, action, player, t, previous_tuple, threshold, agent:Agent):\n",
        "        \"\"\" Action is a int, and player is either 0(agent) or 1(opponent)\n",
        "        \"\"\"\n",
        "        s = self.get_enumerate_states(self.form_state(), threshold, agent)\n",
        "\n",
        "\n",
        "        self.calulate_chips()\n",
        "        if (not 1 in self.agents_chips): #if agent has no chips available\n",
        "            return self.form_state(), 0, True  #0 is a magic number, for a bad reward\n",
        "        done = False\n",
        "        p = self.mana\n",
        "\n",
        "        done, a = self.game.step(action, player) #a is the action aw it was translated from the game\n",
        "        #if opponent is playing, then store its move\n",
        "        self.last_opponent_move = a if player == 1 else  self.last_opponent_move\n",
        "\n",
        "\n",
        "        if (player == 0 and a == 2):#in case that the agent raises\n",
        "            #then I should reduce the agents chips\n",
        "            self.agents_chips[self.chip_index] = 0\n",
        "            if self.chip_index-2 >= 0:\n",
        "                self.calulate_chips()\n",
        "            else:\n",
        "                return self.form_state(), 0, True  #0 is a magic number, for a bad reward\n",
        "        if(player != p and action == 2): #in case that the last player raise (first player is always the mana)\n",
        "            next_player = self.agent if player == 1 else self.opponent\n",
        "            #to do\n",
        "            if isinstance(self.opponent, Random_Agent): enumarated_state=convert_flop_state_to_num(convert_pre_flop_state_to_num(self.form_state()[:5]),self.form_state())\n",
        "            else: enumarated_state=threshold_convert_state_to_num(self.form_state())\n",
        "\n",
        "            if(self.disp):print(\"Re-raise was performed\")\n",
        "            new_action = next_player.send_action(enumarated_state)\n",
        "\n",
        "\n",
        "            #done, a= self.game.step(new_action, np.abs(player-1))\n",
        "            return self.step(new_action, np.abs(player - 1), t, previous_tuple, threshold=threshold, agent=agent)\n",
        "\n",
        "        self.table = self.game.table\n",
        "        state = self.form_state()\n",
        "\n",
        "        if done: #the episode is terminated so I have to send reward\n",
        "\n",
        "            bank_after_episode = self.game.total_money_per_player\n",
        "            reward = bank_after_episode[0] - self.bank[0] #reward is how much money did the agent win (or lose)\n",
        "            self.bank = bank_after_episode\n",
        "            #changing the chips of the agent based on the result of the game\n",
        "            self.calulate_chips()\n",
        "\n",
        "            return state, reward, done\n",
        "        else:\n",
        "            return state, 0, done\n",
        "\n",
        "\n",
        "    def form_state(self):\n",
        "        \"\"\"\n",
        "            state: [0:4 is agents hand (T to A), 5:9 cards on the table (T to A),\n",
        "            10:28 available chips of agent (every index is .5 chips),\n",
        "            29:31 is the laste move of the opponent]\n",
        "        \"\"\"\n",
        "        table_state = [0]*5\n",
        "        table = [\"T\", \"J\", \"Q\", \"K\", \"A\"] #important line in order to create the correct state vector\n",
        "        self.table = self.game.table\n",
        "        for card in self.table:\n",
        "            tmp = list([1 if card.rank == i else 0 for i in table])\n",
        "            table_state = np.bitwise_or(np.array(table_state), np.array(tmp))\n",
        "        last_op_move = [0]*3\n",
        "        if self.game.opponent_last_action is not None:\n",
        "            last_op_move[self.game.opponent_last_action] = 1\n",
        "\n",
        "        return self.agents_hand + table_state.tolist() + last_op_move\n",
        "        #return [self.agents_hand, self.table, self.agents_chips,last_op_move] in case of Q-learning\n",
        "\n",
        "    \"\"\"method only for debugging reasons\"\"\"\n",
        "    def get_enumerate_states(self, state, threshold, agent):\n",
        "        state = self.form_state()\n",
        "\n",
        "        s = return_state( state_vector=state,\n",
        "                                threshold=threshold,\n",
        "                                agent=agent,\n",
        "                                preflop_state = convert_pre_flop_state_to_num(state[0:5])\n",
        "                                )\n",
        "        return s\n",
        "\n"
      ],
      "metadata": {
        "id": "nBO6aROng4io"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main\n"
      ],
      "metadata": {
        "id": "Yrd4dM6Bjlfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd data\n",
        "!mkdir data/rewards/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcL1PaZer5Rp",
        "outputId": "82c81342-17e2-4dc8-dc0c-3330620edefe"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data/rewards/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd images\n",
        "!mkdir images/aggregate/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaWLnB4GsvGz",
        "outputId": "08ff5f08-8f4f-4681-9ea0-93bfadf04fa1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘images/aggregate/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def play_a_game(env: Env, agent: Agent, opponent:Agent, threshold=False, t = None, disp = False, full_stages=False):\n",
        "    games = 0\n",
        "    total_reward = 0\n",
        "\n",
        "    if(t>20_000 ):\n",
        "        agent.reduce_a()\n",
        "\n",
        "\n",
        "    while True: #play as many hands until one player bankrupts\n",
        "        state, *_ = env.reset(disp = disp)\n",
        "        if(isinstance(opponent, Human_Agent)):\n",
        "            #prints in order to inform the human agent\n",
        "            print(\"---------------------New Hand--------------------\")\n",
        "            print(f\"Human_agent's total money{env.game.total_money_per_player[1]}\")\n",
        "            print(f\"Q_learning_agent's total money{env.game.total_money_per_player[0]}\")\n",
        "        if(not isinstance(opponent, Random_Agent)): opponent.set_hand(env.game.hand_of_player[1])\n",
        "        games += 1\n",
        "        if state == -1: return total_reward #means that the game has ended\n",
        "        state = convert_pre_flop_state_to_num(state[0:5]) if not threshold else threshold_convert_state_to_num(state)\n",
        "        preflop_state = state\n",
        "        done = False\n",
        "        mana = env.mana\n",
        "        #for q_learning\n",
        "        prev_state = state\n",
        "        action = None\n",
        "        reward = 0\n",
        "        done = False\n",
        "        round = 1\n",
        "        while not done: #playing one hand\n",
        "            #remember that state, reward an done are referring only in the agent\n",
        "            round = (round + 1 )%2\n",
        "            if (not isinstance(opponent, Random_Agent)): #instance of human or threshold agent\n",
        "                opponent.set_round(round)\n",
        "                opponent.set_table(env.game.table)\n",
        "            previous_tuple = [prev_state, action, reward, state, done]\n",
        "\n",
        "            if mana == 0: #if our agent is the mana\n",
        "                prev_state = state\n",
        "                action = agent.send_action(state)\n",
        "                if (isinstance(opponent, Human_Agent)): print(f\"q learning agent's action is {action}\")\n",
        "                state, reward, done=env.step(action, 0, t, previous_tuple, threshold=threshold, agent=agent)\n",
        "                state = return_state(state, threshold, agent,preflop_state, full_stages=full_stages)\n",
        "                if isinstance(agent, Q_Learning_Agent):#Agent is training after its turn\n",
        "                    agent.train([prev_state, action, reward, state, done])\n",
        "\n",
        "                total_reward += reward\n",
        "                if done: break\n",
        "                state, reward, done = env.step(opponent.send_action(state), 1, t, previous_tuple, threshold=threshold, agent=agent)\n",
        "                state = return_state(state, threshold, agent,preflop_state,full_stages=full_stages)\n",
        "                total_reward += reward\n",
        "                if done: break\n",
        "\n",
        "            else:\n",
        "\n",
        "                state, reward, done=env.step(opponent.send_action(state), 1, t, previous_tuple, threshold=threshold, agent=agent)\n",
        "                state = return_state(state, threshold, agent,preflop_state,full_stages=full_stages)\n",
        "                prev_state = state\n",
        "                total_reward += reward\n",
        "                if done: break\n",
        "                action = agent.send_action(state)\n",
        "                if (isinstance(opponent, Human_Agent)): print(f\"q learning agent's action is {action}\")\n",
        "                state, reward, done=env.step(action, 0, t, previous_tuple, threshold=threshold, agent=agent)\n",
        "                state = return_state(state, threshold, agent,preflop_state,full_stages=full_stages)\n",
        "                if isinstance(agent, Q_Learning_Agent):#Agent is training after its turn\n",
        "                    agent.train([prev_state, action, reward, state, done])\n",
        "\n",
        "                total_reward += reward\n",
        "                if done: break\n",
        "\n",
        "            if round == 0:#time to inform the opponents about the cards on the table\n",
        "                env.game.table = [env.game.dealer.deal_card(),env.game.dealer.deal_card()]\n",
        "                #i have to update in this step the state\n",
        "                state = env.form_state()\n",
        "                state = return_state(state, threshold, agent,preflop_state,full_stages=full_stages)\n",
        "\n",
        "\n",
        "\n",
        "def training_main(threshold, q_learning, aggressive):\n",
        "    threshold = threshold\n",
        "    q_learning = q_learning\n",
        "    p = P_THRESHOLD_A if threshold and aggressive else \\\n",
        "                (P_THRESHOLD_D if threshold and not aggressive else P)\n",
        "    seed = 15\n",
        "    np.random.seed(seed)\n",
        "    gamma = .9\n",
        "    agent = Q_Learning_Agent(   #state_size=2**10,\n",
        "                                state_size=20 if not threshold else 33,\n",
        "                                action_size= 3,\n",
        "                                a=.4 ,\n",
        "                                gamma=gamma,\n",
        "                                against_human = False) if q_learning else \\\n",
        "                                PolicyIterationAgent(P=p, gamma=gamma)\n",
        "    if threshold :\n",
        "        opponent = Threshold_Agent_A() if aggressive else Threshold_Agent_D()\n",
        "    else:\n",
        "        opponent = Random_Agent(seed = seed)\n",
        "        aggressive = False #just for constistency in the csv data\n",
        "\n",
        "    horizon = 80_000\n",
        "\n",
        "    env = Env(agent, opponent, number_of_cards=5, seed=np.random.randint(low=1, high = horizon))\n",
        "    r = np.zeros(horizon)\n",
        "    reward = np.zeros(horizon)\n",
        "\n",
        "    for t in tqdm(range(horizon), desc=\"Processing items\", unit=\"item\"):\n",
        "\n",
        "        reward[t] = play_a_game(env,agent,opponent, threshold=threshold, t=t, disp = False, full_stages=False)\n",
        "        r[t] = reward[t]+r[t-1]*(t>0)\n",
        "        s = np.random.randint(low = 1, high = horizon) #new seed in order to play a different sequence of card in every episode\n",
        "        env = Env(agent, opponent, number_of_cards=5, seed=s)\n",
        "\n",
        "    print(f\"Total reward mean for the last 1000 iterations is {np.mean(reward[-1:-1000:-1])}\")\n",
        "    #data saving for report\n",
        "    #saving the policy\n",
        "    if not q_learning:\n",
        "        decisions = list([agent.pi(i) for i in range(33 if threshold else 20)])\n",
        "        np.savetxt(f\"./data/q_learning_{ q_learning}_threshold_{threshold}_aggressive_{aggressive}.csv\", decisions)\n",
        "    else:\n",
        "        decisions = list([np.argmax(agent.Q[i,:]) for i in range(33 if threshold else 20)])\n",
        "        np.savetxt(f\"./data/q_learning_{ q_learning}_threshold_{threshold}_aggressive_{aggressive}.csv\", decisions)\n",
        "        np.savetxt(\"./data/q_agent.csv\", agent.Q, delimiter = ',')\n",
        "    #saving the reward in order to plot it\n",
        "    np.savetxt(f\"./data/rewards/q_learning_{ q_learning}_threshold_{threshold}_aggressive_{aggressive}.csv\", r)\n",
        "\n",
        "    #kind of debugging plots\n",
        "    plt.figure(1)\n",
        "    plt.title(f\" Agent's Reward \")\n",
        "    plt.xlabel(\"Round T\")\n",
        "    plt.ylabel(\"Total Score\")\n",
        "    plt.plot(np.arange(1,horizon+1),r, label=\"Cumulative Reward\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.savefig(f'images/q_learning_{ q_learning}_threshold_{threshold}_aggressive_{aggressive}.jpg')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def testing_main():\n",
        "\n",
        "    \"\"\" Method that tests the pre-trained learning agent against a human\"\"\"\n",
        "\n",
        "    horizon = 2\n",
        "    #loading the pre-trained agent\n",
        "    q = np.loadtxt(\"./data/q_agent.csv\", delimiter=\",\",dtype = float)\n",
        "    agent = Q_Learning_Agent(state_size = 33, action_size = 2, Q = q, against_human=True )\n",
        "\n",
        "    opponent = Human_Agent(action_size=2)\n",
        "    env = Env(agent, opponent, number_of_cards=5, seed=np.random.randint(low=1, high = horizon))\n",
        "\n",
        "    #It follows the same implementation as the training algorithm\n",
        "    r = np.zeros(horizon)\n",
        "    reward = np.zeros(horizon)\n",
        "    for t in tqdm(range(horizon), desc= \"Processing items\", unit = \"item\"):\n",
        "\n",
        "        reward[t] = play_a_game(env,agent,opponent, threshold=threshold, t=t, disp= True)\n",
        "        r[t] = reward[t]+r[t-1]*(t>0)\n",
        "        s = np.random.randint(low = 1, high = horizon)\n",
        "        env = Env(agent, opponent, number_of_cards=5, seed=s)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    q_learning = True #Use a q-learning agent or not\n",
        "    threshold = True #Use a threshold or a random opponent\n",
        "    aggressive = True #in case of threshold, use aggressive or defensive opponent\n",
        "    train = True\n",
        "    if(train):training_main(threshold = threshold, q_learning = q_learning, aggressive = aggressive)\n",
        "    else: testing_main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "mwafBAS7jm6J",
        "outputId": "02677d08-4f06-4bc1-ba58-571b0b16162c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  27%|██▋       | 21830/80000 [01:26<09:04, 106.91item/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eps = .01------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:  49%|████▉     | 39388/80000 [03:05<03:31, 192.25item/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 0, 2, 2, 1, 0, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items: 100%|██████████| 80000/80000 [06:58<00:00, 191.02item/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward mean for the last 1000 iterations is 4.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}