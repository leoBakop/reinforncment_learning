{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Reinforcement Learning and Dynamic Optimization,\\\n",
        "Poker Playing Agent Project : Second Assignment\n",
        "\n",
        "Group 18:\\\n",
        "Leonidas Bakopoulos AM 2018030036 \\\n",
        "Alexandra Tsipouraki AM 2018030089\n",
        "\n",
        "\n",
        "#Note:\n",
        "In order for a human to play against the (pre-trained) DQN, please run the human evaluation.py in the .py scripts (not in colab). Otherwise the necessery files (weights of the pretained models) will not be available."
      ],
      "metadata": {
        "id": "OjZsQ17KOEIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rlcard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-_37j4CMtg0",
        "outputId": "9a623eac-f6dd-4d9f-fe6f-6e4f25457090"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rlcard\n",
            "  Downloading rlcard-1.2.0.tar.gz (269 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/269.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m194.6/269.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from rlcard) (1.23.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from rlcard) (2.3.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.2.0-py3-none-any.whl size=325793 sha256=c9f7a170be3b97d877424530cfb499958e2011d502e4fb6c2cb201b0eeaf7015\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/39/26d73b035027276e526bec94b0217ed799109d7890c34a7d9b\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard\n",
            "Successfully installed rlcard-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network architecture\n",
        "class DQN_Network(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(DQN_Network, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.2)  # Dropout with 20% probability\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        actions = self.fc3(x)\n",
        "        return actions\n",
        "\n"
      ],
      "metadata": {
        "id": "mr9vMmCWL_BL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer():\n",
        "\n",
        "    def __init__(self, batch_size, buffer_size, device):\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory = deque(maxlen=self.buffer_size)\n",
        "\n",
        "    def add(self, tuple):\n",
        "        for t in tuple:\n",
        "            self.memory.append(t)\n",
        "\n",
        "    def sample(self):\n",
        "        sampled_elements = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        state = list([s[0] for s in sampled_elements])\n",
        "        action = list([s[1] for s in sampled_elements])\n",
        "        reward = list([s[2] for s in sampled_elements])\n",
        "        next_state = list([s[3] for s in sampled_elements])\n",
        "        done = list([s[4] for s in sampled_elements])\n",
        "\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class PriorizedExperienceReplay():\n",
        "    def __init__(self, batch_size, buffer_size, device, alpha, beta):\n",
        "\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory = deque(maxlen=self.buffer_size)\n",
        "        self.priorities = np.zeros(buffer_size, dtype = np.float32)\n",
        "        self.index = 0\n",
        "        self.full = False\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def add(self, tuple):\n",
        "        for t in tuple:\n",
        "            self.memory.append(t)\n",
        "            #The new tuple must be selected at least the first time\n",
        "            self.priorities[self.index] = 1 if not self.full and self.index == 0 \\\n",
        "                                            else self.priorities.max()\n",
        "            self.index = (self.index + 1) % self.buffer_size\n",
        "            self.full = len(self.memory) == self.buffer_size\n",
        "\n",
        "    def sample(self):\n",
        "        if self.full:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.index]\n",
        "\n",
        "        # calc P = p^a/sum(p^a)\n",
        "        probs  = prios ** self.alpha\n",
        "        P = probs/probs.sum()\n",
        "\n",
        "        #gets the indices depending on the probability p\n",
        "        indices = np.random.choice(len(self.memory), self.batch_size, p=P)\n",
        "        sampled_elements = [self.memory[idx] for idx in indices]\n",
        "\n",
        "\n",
        "\n",
        "        #Compute importance-sampling weight\n",
        "        weights  = (len(self.memory) * P[indices]) ** (-self.beta)\n",
        "        # normalize weights\n",
        "        weights /= weights.max()\n",
        "        weights  = np.array(weights, dtype=np.float32)\n",
        "\n",
        "        state = list([s[0] for s in sampled_elements])\n",
        "        action = list([s[1] for s in sampled_elements])\n",
        "        reward = list([s[2] for s in sampled_elements])\n",
        "        next_state = list([s[3] for s in sampled_elements])\n",
        "        done = list([s[4] for s in sampled_elements])\n",
        "\n",
        "        return state, action, reward, next_state, done, indices, weights\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def update_priorities(self, batch_indices, batch_priorities):\n",
        "        for idx, prio in zip(batch_indices, batch_priorities):\n",
        "            self.priorities[idx] = prio\n",
        "\n",
        "    def set_alpha(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def set_beta(self, beta):\n",
        "        self.beta = beta"
      ],
      "metadata": {
        "id": "O7ZKk3joMFY8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mzUxH1kvLwgL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Agent():\n",
        "    def __init__(   self,\n",
        "                    input_size,\n",
        "                    hidden_size1,\n",
        "                    hidden_size2,\n",
        "                    num_actions,\n",
        "                    device,\n",
        "                    batch_size = 256,\n",
        "                    buffer_size = 10_000,\n",
        "                    gamma = .99,\n",
        "                    horizon = 1_000_000,\n",
        "                    lr = .001,\n",
        "                    decrease = .99,\n",
        "                    goal = .02,\n",
        "                    per = False,\n",
        "                    a = 0,\n",
        "                    b = 0\n",
        "                ):\n",
        "        self.device = device\n",
        "\n",
        "        #networks\n",
        "        self.num_actions= num_actions\n",
        "        self.model = DQN_Network(input_size, hidden_size1, hidden_size2, num_actions).to(self.device)\n",
        "        self.target_model = DQN_Network(input_size, hidden_size1, hidden_size2, num_actions).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "        #memory staffs\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.per = per\n",
        "        if not per:\n",
        "            self.replay_buffer = ReplayBuffer(batch_size= self.batch_size,buffer_size= self.buffer_size,device= self.device)\n",
        "        else:\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.offset = .05\n",
        "            self.replay_buffer = PriorizedExperienceReplay(batch_size= self.batch_size,buffer_size= self.buffer_size,device= self.device, alpha = a, beta = b)\n",
        "\n",
        "\n",
        "        #miscellaneous\n",
        "        self.gamma = gamma\n",
        "        self.horizon = horizon\n",
        "        self.lr = lr\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.use_raw = False #just for the env, False because the agent is not a human\n",
        "        self.counter = 0\n",
        "        self.TAU = .005\n",
        "        #epsilon greedy part\n",
        "        self.eps = 1.0\n",
        "        self.decrease = decrease\n",
        "        self.goal = goal\n",
        "        self.dt = .001\n",
        "        self.epsilon_values = np.linspace(1.0, goal+self.dt, self.decrease)\n",
        "        self.index = 0\n",
        "\n",
        "    def push(self, tuples):\n",
        "        self.replay_buffer.add(tuples)\n",
        "\n",
        "\n",
        "    def no_grad_predict(self,state, network):\n",
        "\n",
        "        ''' Predict the masked Q-values\n",
        "\n",
        "        Args:\n",
        "            state (numpy.array): current state,\n",
        "            the network that is going to use,\n",
        "            model (boolean): defines if the model or the model network is used (in case of training) in order to\n",
        "            use or not grand,\n",
        "\n",
        "        Returns:\n",
        "            q_values (numpy.array): a 1-d array where each entry represents a Q value and sets -inf to the illegal\n",
        "        '''\n",
        "        training = len(state) != 5\n",
        "        network.eval()\n",
        "        #we should remember that state['obs'] is the 72 (or more in case of an extended environment) vector\n",
        "        if not training: #in case that there is just one tuple\n",
        "            input = torch.Tensor(np.expand_dims(state['obs'], 0)).to(self.device)\n",
        "        else:\n",
        "            new_state_obs = []\n",
        "            new_state_legal_action = []\n",
        "            for s in state:\n",
        "                new_state_obs.append(s['obs'])\n",
        "                new_state_legal_action.append(list(s['legal_actions'].keys()))\n",
        "            input = torch.Tensor(np.array(new_state_obs)).to(self.device)\n",
        "        #taking all the q-values\n",
        "        with torch.no_grad():\n",
        "            q_values = network(input)[0].cpu().detach().numpy() if not training else network(input).cpu().detach().numpy()\n",
        "\n",
        "        network.train()\n",
        "        #mask the illegal actions\n",
        "        masked_q_values = -np.inf * np.ones(self.num_actions, dtype=float) if not training else -np.inf*np.ones((self.batch_size, self.num_actions), dtype = float)\n",
        "        #I want the keys not the values and I have implement the values\n",
        "        legal_actions = list(state['legal_actions'].keys()) if len(state) == 5 else list(new_state_legal_action)\n",
        "        if training:\n",
        "            for i,(m,q) in enumerate(zip(masked_q_values, q_values)):\n",
        "                m[legal_actions[i]] = q[legal_actions[i]] #replace the -infinity with the true value when an action is legal\n",
        "        else:\n",
        "            masked_q_values[legal_actions] = q_values[legal_actions]\n",
        "\n",
        "        return masked_q_values\n",
        "\n",
        "    def eval_step(self,state):\n",
        "        \"\"\"\n",
        "        method required from the rl-card environment.\n",
        "        This method is called in env.run(is_training = False)\n",
        "        and returns a clear action.\n",
        "        \"\"\"\n",
        "        qs = self.no_grad_predict(state, network = self.model)\n",
        "        action = np.argmax(qs)\n",
        "\n",
        "        return action, None\n",
        "\n",
        "    def step(self, state):\n",
        "        \"\"\"\n",
        "        method required from the rl-card environment.\n",
        "        This method is called in env.run(is_training = True)\n",
        "        and returns an action, selected by eps-greedy.\n",
        "        \"\"\"\n",
        "        self.update_eps()\n",
        "        p = random.random()\n",
        "        legal_actions = list(state['legal_actions'].keys())\n",
        "\n",
        "        if p < self.eps: #return random move\n",
        "            return legal_actions[random.randint(a = 0, b =len(legal_actions)-1)]\n",
        "\n",
        "        q_values = self.no_grad_predict(state, network = self.model)\n",
        "\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "\n",
        "    def agents_step(self, tuples):\n",
        "        \"\"\"\n",
        "        method responsible for storing the new experience in replay buffer,\n",
        "        and train the agent. Basically the method must be called in every timestep\n",
        "        of the training loop\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        #stores new experience in replay buffer\n",
        "        self.push(tuples)\n",
        "        if len(self.replay_buffer) < 2*self.batch_size: return\n",
        "\n",
        "        #enough experience was stored, so I can sample a minibatch\n",
        "        experience = self.replay_buffer.sample()\n",
        "        self.set_per_values()\n",
        "        #now it is time for training\n",
        "        if not self.per:self.train(experience)\n",
        "        else: self.train_per(experience)\n",
        "\n",
        "\n",
        "\n",
        "    def update_eps(self):\n",
        "        if self.eps > self.goal+self.dt:\n",
        "            self.index +=  1\n",
        "            self.eps = self.epsilon_values[self.index]\n",
        "\n",
        "        if (self.eps == self.goal + self.dt):\n",
        "            print(\"\\n----------exploration ended--------------\")\n",
        "            self.eps = self.goal - self.dt\n",
        "            #self.optimizer.param_groups[0]['lr'] = self.lr*0.1\n",
        "\n",
        "    def train_per(self,experience):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        state, action, reward, next_state, done, idx, weights = experience\n",
        "        action = torch.tensor(action).to(self.device)\n",
        "        legal_actions_batch = list([ns['legal_actions'] for ns in next_state])\n",
        "\n",
        "        #calulating the max(Q(s',a'))\n",
        "        next_qs = self.no_grad_predict(state = next_state, network = self.target_model)\n",
        "\n",
        "        legal_actions = []\n",
        "        for b in range(self.batch_size):\n",
        "            legal_actions.extend([i + b * self.num_actions for i in legal_actions_batch[b]])\n",
        "\n",
        "        #masking the illegal moves for Q(s',a')\n",
        "        masked_q_values = -np.inf * np.ones(self.num_actions * self.batch_size, dtype=float)\n",
        "        masked_q_values[legal_actions] = next_qs.flatten()[legal_actions]\n",
        "        masked_q_values = masked_q_values.reshape((self.batch_size, self.num_actions))\n",
        "        #calculating the best action based in the Q(s', a')\n",
        "        best_actions = np.argmax(masked_q_values, axis=1)\n",
        "\n",
        "        #calulating the target\n",
        "        done = list(map(float, done))\n",
        "        ones= np.ones_like(done)\n",
        "        y = reward + self.gamma* next_qs[np.arange(self.batch_size), best_actions]*(ones-done)\n",
        "        y = torch.tensor(y, dtype = torch.float32).to(self.device)\n",
        "        #so y = rewards + gamma* max(Q(s',a')) * done\n",
        "        #calulating the Q(s,a) using the model network\n",
        "        state = list([s['obs'] for s in state])\n",
        "        state = torch.Tensor(np.array(state)).to(self.device)\n",
        "        qs = self.model(state) #calulating the Q(s,a) for every a\n",
        "        Q = torch.gather(qs, dim=-1, index=action.unsqueeze(-1)).squeeze(-1).to(self.device) #filtering the selected a\n",
        "\n",
        "\n",
        "        #It's time for training\n",
        "        w = torch.Tensor((weights**(1-self.b))).to(self.device)\n",
        "        loss = (self.criterion(Q, y)*w).mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update()\n",
        "        self.model.eval()\n",
        "\n",
        "        #updating the propabillities\n",
        "        td_error =  Q - y\n",
        "        difference = td_error + self.offset\n",
        "        self.replay_buffer.update_priorities(idx, abs(difference))\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def train(self,experience):\n",
        "\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        state, action, reward, next_state, done = experience\n",
        "        action = torch.tensor(action).to(self.device)\n",
        "        legal_actions_batch = list([ns['legal_actions'] for ns in next_state])\n",
        "\n",
        "        #calulating the max(Q(s',a'))\n",
        "        next_qs = self.no_grad_predict(state = next_state, network = self.target_model)\n",
        "\n",
        "        legal_actions = []\n",
        "        for b in range(self.batch_size):\n",
        "            legal_actions.extend([i + b * self.num_actions for i in legal_actions_batch[b]])\n",
        "\n",
        "        #masking the illegal moves for Q(s',a')\n",
        "        masked_q_values = -np.inf * np.ones(self.num_actions * self.batch_size, dtype=float)\n",
        "        masked_q_values[legal_actions] = next_qs.flatten()[legal_actions]\n",
        "        masked_q_values = masked_q_values.reshape((self.batch_size, self.num_actions))\n",
        "        #calculating the best action based in the Q(s', a')\n",
        "        best_actions = np.argmax(masked_q_values, axis=1)\n",
        "\n",
        "        #calulating the target\n",
        "        done = list(map(float, done))\n",
        "        ones= np.ones_like(done)\n",
        "        y = reward + self.gamma* next_qs[np.arange(self.batch_size), best_actions]*(ones-done)\n",
        "        y = torch.tensor(y, dtype = torch.float32).to(self.device)\n",
        "        #so y = rewards + gamma* max(Q(s',a')) * done\n",
        "        #calulating the Q(s,a) using the model network\n",
        "        state = list([s['obs'] for s in state])\n",
        "        state = torch.Tensor(np.array(state)).to(self.device)\n",
        "        qs = self.model(state) #calulating the Q(s,a) for every a\n",
        "        Q = torch.gather(qs, dim=-1, index=action.unsqueeze(-1)).squeeze(-1) #filtering the selected a\n",
        "\n",
        "\n",
        "        #It's time for training\n",
        "        loss = self.criterion(Q,y)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update()\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    def soft_update(self):\n",
        "\n",
        "        for target_param, local_param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n",
        "\n",
        "    def set_per_values(self):\n",
        "        if not self.per: return\n",
        "        self.replay_buffer.set_alpha(1-self.eps)\n",
        "        self.replay_buffer.set_beta(1-self.eps)\n",
        "\n",
        "\n",
        "    def load_model(self, weights):\n",
        "        self.model.load_state_dict(weights)\n",
        "        self.target_model.load_state_dict(weights)\n",
        "        self.model.eval()\n",
        "        self.target_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "br = 0 #stands for bad reward\n",
        "mr = 1 #stands for medium reward\n",
        "gr = 2 #stands for good reward\n",
        "\n",
        "#dictionary that represents the value of each card\n",
        "cards = {\n",
        "    \"2\": br,\n",
        "    \"3\": br,\n",
        "    \"4\": br,\n",
        "    \"5\": br,\n",
        "    \"6\": br,\n",
        "    \"7\": mr,\n",
        "    \"8\": mr,\n",
        "    \"9\": mr,\n",
        "    \"10\": mr,\n",
        "    \"J\": gr,\n",
        "    \"Q\": gr,\n",
        "    \"K\": gr,\n",
        "    \"A\": gr\n",
        "}\n",
        "\n",
        "action_hierarchy = {\"raise\":1, \"call\":0, \"check\":3, \"fold\":2}\n",
        "\n",
        "def get_action(legal_moves, desired_move):\n",
        "    \"\"\"\n",
        "        method that gets the -enumerated- desired action and returns the\n",
        "        closest one(based on poker criteria and the so-called'legal' moves).\n",
        "        returns 2 (fold) in case of an error\n",
        "    \"\"\"\n",
        "\n",
        "    action_hierarchy_list = list(action_hierarchy.keys())\n",
        "    index = action_hierarchy_list.index(desired_move)\n",
        "    if desired_move in legal_moves: return action_hierarchy[desired_move]\n",
        "    for i in range(3):\n",
        "        next_move = action_hierarchy_list[(index+i+1)%len(action_hierarchy_list)]\n",
        "        if next_move in legal_moves: return action_hierarchy.get(next_move, 2)\n",
        "    return 2\n",
        "\n",
        "class Threshold_Agent():\n",
        "    #aka the offensive/loose agent\n",
        "    def __init__(self):\n",
        "        self.use_raw = False #just for the env, False because the agent is not a human\n",
        "        self.pair = False\n",
        "\n",
        "    def step(self, state):\n",
        "        retval=self.eval_step(state)[0]\n",
        "        return retval\n",
        "\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        \"\"\"\n",
        "            method that sends/decides the action of the layer.\n",
        "        \"\"\"\n",
        "\n",
        "        hand = self.get_hand(state)\n",
        "        table = self.get_table(state)\n",
        "        if table == -1: return self.play_preflop(state, hand), None\n",
        "        return self.play_post_flop(state, hand, table), None\n",
        "\n",
        "\n",
        "\n",
        "    def get_hand(self, state):\n",
        "        hand = state[\"raw_obs\"][\"hand\"]\n",
        "        hand=list([list(h)[-1]for h in hand]) #hand variable contains the rank of the cards\n",
        "        return hand\n",
        "\n",
        "    def get_table(self, state):\n",
        "        table = state[\"raw_obs\"].get(\"table\", False) #return false in case of preflop\n",
        "        if(not table): return -1\n",
        "        table=list([list(h)[-1]for h in table]) #table variable contains the rank of the cards\n",
        "        return table\n",
        "\n",
        "    def play_preflop(self, state,hand):\n",
        "        \"\"\"\n",
        "        check if agent has strong preflop hand.\n",
        "        ARGUMENTS: the ranks of the hand, state as the environment returns\n",
        "        \"\"\"\n",
        "        pair = hand[0] == hand[1]\n",
        "        self.pair = pair\n",
        "        value = cards.get(hand[0], 0) + cards.get(hand[1], 0)\n",
        "        legal_actions = state[\"raw_legal_actions\"]\n",
        "        if pair or value >= 3:\n",
        "            return get_action(legal_moves=legal_actions, desired_move = \"raise\")\n",
        "        return get_action(legal_moves=legal_actions, desired_move = \"call\")\n",
        "\n",
        "    def play_post_flop(self, state, hand, table):\n",
        "        \"\"\"\n",
        "        Decides either to raise in case of a match on the table,\n",
        "        or to just call\n",
        "        Arguments: ranked (hand and table)\n",
        "        \"\"\"\n",
        "        match = False\n",
        "        legal_actions = state[\"raw_legal_actions\"]\n",
        "\n",
        "        for h in hand:\n",
        "            match = True if h in table else match\n",
        "        #in case of a match within the hand and the table or just a pair in hand\n",
        "        if match or self.pair: return get_action(legal_moves = legal_actions, desired_move=\"raise\")\n",
        "        return get_action(legal_moves = legal_actions, desired_move=\"call\")\n",
        "\n",
        "\n",
        "class Tight_Threshold_Agent(Threshold_Agent):\n",
        "\n",
        "    def play_preflop(self, state,hand):\n",
        "        \"\"\"\n",
        "        check if agent has strong preflop hand.\n",
        "        ARGUMENTS: the ranks of the hand, state as the environment returns\n",
        "        \"\"\"\n",
        "        pair = hand[0] == hand[1]\n",
        "        self.pair = pair\n",
        "        value = cards.get(hand[0], 0) + cards.get(hand[1], 0)\n",
        "        legal_actions = state[\"raw_legal_actions\"]\n",
        "        if pair or value >= 3:\n",
        "            return get_action(legal_moves=legal_actions, desired_move = \"call\")\n",
        "        return get_action(legal_moves=legal_actions, desired_move = \"check\")\n",
        "\n",
        "\n",
        "    def play_post_flop(self, state, hand, table):\n",
        "        \"\"\"\n",
        "        Decides either to raise in case of a match on the table,\n",
        "        or to just call\n",
        "        Arguments: ranked (hand and table)\n",
        "        \"\"\"\n",
        "        match = False\n",
        "        legal_actions = state[\"raw_legal_actions\"]\n",
        "\n",
        "        for h in hand:\n",
        "            match = True if h in table else match\n",
        "        #in case of a match within the hand and the table or just a pair in hand\n",
        "        if match or self.pair: return get_action(legal_moves = legal_actions, desired_move=\"call\")\n",
        "        return get_action(legal_moves = legal_actions, desired_move=\"check\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mqCX2qajMR--"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rlcard\n",
        "import torch\n",
        "from rlcard.agents import RandomAgent\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from rlcard.models.limitholdem_rule_models import LimitholdemRuleAgentV1\n",
        "\n",
        "from rlcard.utils import (\n",
        "    set_seed,\n",
        "    tournament,\n",
        "    reorganize\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed = 42\n",
        "    env = rlcard.make(\"limit-holdem\", config={'seed': seed,})\n",
        "    set_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"used device is {device}\")\n",
        "    horizon = 3_000_000\n",
        "    num_eval_games = 2_000 #how many hands will be played in every tournament\n",
        "    evaluate_every = 100_000\n",
        "    index = 0\n",
        "\n",
        "    threshold = True\n",
        "    per = False\n",
        "    loose = False\n",
        "    best_threshold = False\n",
        "\n",
        "    threshold = True if best_threshold else threshold\n",
        "    loose =False if not threshold else loose\n",
        "\n",
        "\n",
        "    agent = Agent(\n",
        "        input_size= env.state_shape[0][0],\n",
        "        hidden_size1= 512,\n",
        "        hidden_size2=256,\n",
        "        num_actions=env.num_actions,\n",
        "        device=device,\n",
        "        batch_size=64,\n",
        "        buffer_size=100_000,\n",
        "        gamma = .99,\n",
        "        lr = 10**(-5), #good lr is .00003\n",
        "        decrease= int(2*1.7*0.4*horizon), #exploration in the 40% of the horizon. # because the agent's step is called almost 1.7 times ine evry game\n",
        "        goal = .1,\n",
        "        per = per\n",
        "    )\n",
        "\n",
        "    agents=[agent]\n",
        "    for _ in range(1, env.num_players):\n",
        "        if threshold:\n",
        "            opp = Threshold_Agent() if loose else Tight_Threshold_Agent()\n",
        "            opp = LimitholdemRuleAgentV1() if best_threshold else opp\n",
        "        else:\n",
        "            opp = RandomAgent(num_actions=env.num_actions)\n",
        "        agents.append(opp)\n",
        "    print(f\"the opponent of the agent is {type(opp)}\")\n",
        "    env.set_agents(agents)\n",
        "\n",
        "    rewards = np.zeros(int(horizon/evaluate_every))\n",
        "    for episode in tqdm(range(horizon), desc=\"Processing items\", unit=\"item\"):\n",
        "\n",
        "        # Generate data from the environment\n",
        "        trajectories, payoffs = env.run(is_training=True)\n",
        "        # Reorganaize the data to be state, action, reward, next_state, done\n",
        "        trajectories = reorganize(trajectories, payoffs)\n",
        "        agent.agents_step(trajectories[0])\n",
        "\n",
        "        #logistics/evaluation on clear data\n",
        "        if episode%evaluate_every == 0 and index < len(rewards):\n",
        "            rewards[index] = tournament(env,num_eval_games)[0]\n",
        "            index+=1\n",
        "\n",
        "    print(f\"the buffer size at the end is {len(agent.replay_buffer)}\")\n",
        "    file_path = f\"./data/final/\"\n",
        "    if not os.path.exists(file_path):\n",
        "    # If it doesn't exist, create the directory\n",
        "        os.makedirs(file_path)\n",
        "        print(f\"Directory '{file_path}' has been created.\")\n",
        "    else:\n",
        "        print(f\"Directory '{file_path}' already exists.\")\n",
        "    torch.save(agent.model.state_dict(), file_path+f'models/threshold_{threshold}_per_{per}_loose_{loose}_best_{best_threshold}_model.pth')\n",
        "    np.save(file_path+f\"threshold_{threshold}_per_{per}_loose_{loose}_best_{best_threshold}.npy\", rewards, allow_pickle=True)\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.title(f\" Agent's Reward \")\n",
        "    plt.xlabel(\"Round T\")\n",
        "    plt.ylabel(\"Average Score\")\n",
        "    plt.plot(np.linspace(0, horizon, int(horizon/evaluate_every)),rewards, label=\"Average reward per episode\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"./images/threshold_{threshold}_per_{per}_loose_{loose}_best_{best_threshold}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6dB9h-V5MUQg",
        "outputId": "e6144cc4-3787-4233-ab46-608dbfec327f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "used device is cpu\n",
            "the opponent of the agent is <class '__main__.Tight_Threshold_Agent'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing items:   0%|          | 938/3000000 [00:14<12:31:37, 66.50item/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-124d29872e8b>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Reorganaize the data to be state, action, reward, next_state, done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorganize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayoffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#logistics/evaluation on clear data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-abe52f04da6c>\u001b[0m in \u001b[0;36magents_step\u001b[0;34m(self, tuples)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_per_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m#now it is time for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_per\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-abe52f04da6c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m#calulating the max(Q(s',a'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mnext_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mlegal_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-abe52f04da6c>\u001b[0m in \u001b[0;36mno_grad_predict\u001b[0;34m(self, state, network)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#taking all the q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e87f18fa9201>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}